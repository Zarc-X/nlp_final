"""
è‡ªæˆ‘æ¼”åŒ–æ ¸å¿ƒé€»è¾‘æ¨¡å—
"""
import torch
import json
import time
from typing import Dict, List, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
import os

from config import (
    EVOLUTION_CONFIG, 
    API_CONFIG, 
    TRAINING_DATA_DIR, 
    CHECKPOINT_DIR
)
from core.model_manager import get_model
from core.api_client import call_qwen_api, validate_code_with_14b
from core.code_processor import check_code_syntax, run_simple_test
from data.training_data import save_training_example
from utils.text_utils import detect_evolution_mode, extract_problems_from_text
from utils.progress_tracker import create_progress_tracker, update_progress


def process_single_problem(problem: str, system_prompt: str = None) -> Tuple[bool, Dict]:
    """
    å¤„ç†å•ä¸ªé—®é¢˜çš„å®Œæ•´æµç¨‹
    """
    result = {
        "problem": problem,
        "success": False,
        "generated_code": "",
        "validation_result": "",
        "test_result": "",
        "saved_file": ""
    }
    
    # æ­¥éª¤1: ä½¿ç”¨70Bæ¨¡å‹ç”Ÿæˆä»£ç 
    success, code = call_qwen_api(
        API_CONFIG["qwen_70b_api_url"],
        problem,
        model_name="Qwen2.5-Coder-70B"
    )
    
    if not success:
        result["validation_result"] = f"ä»£ç ç”Ÿæˆå¤±è´¥: {code}"
        return False, result
    
    result["generated_code"] = code
    
    # æ­¥éª¤2: è¯­æ³•æ£€æŸ¥
    syntax_ok, syntax_msg = check_code_syntax(code)
    if not syntax_ok:
        result["validation_result"] = f"è¯­æ³•é”™è¯¯: {syntax_msg}"
        return False, result
    
    # æ­¥éª¤3: é€»è¾‘éªŒè¯ï¼ˆ14Bæ¨¡å‹ï¼‰
    logic_ok, logic_msg = validate_code_with_14b(problem, code)
    result["validation_result"] = logic_msg
    
    if not logic_ok:
        return False, result
    
    # æ­¥éª¤4: è¿è¡Œç®€å•æµ‹è¯•
    test_ok, test_msg = run_simple_test(code, problem)
    result["test_result"] = test_msg
    
    if not test_ok:
        print(f"æµ‹è¯•å¤±è´¥ï¼Œä½†ä»ä¿å­˜ç¤ºä¾‹: {test_msg}")
    
    # æ­¥éª¤5: ä¿å­˜è®­ç»ƒæ•°æ®
    try:
        saved_file = save_training_example(problem, code, logic_msg)
        result["saved_file"] = saved_file
        result["success"] = True
    except Exception as e:
        result["validation_result"] += f"\nä¿å­˜å¤±è´¥: {str(e)}"
        return False, result
    
    return True, result


def fine_tune_on_examples(examples: List[Dict]) -> str:
    """
    åœ¨å¤šä¸ªç¤ºä¾‹ä¸Šå¾®è°ƒæ¨¡å‹
    """
    model, tokenizer, device = get_model()
    
    if model is None or tokenizer is None:
        return "æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•è¿›è¡Œå¾®è°ƒ"
    
    if not examples:
        return "æ²¡æœ‰æœ‰æ•ˆçš„è®­ç»ƒç¤ºä¾‹"
    
    try:
        model.train()
        
        total_loss = 0
        successful_updates = 0
        
        for example in examples:
            try:
                # å‡†å¤‡è®­ç»ƒæ•°æ®
                messages = [
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¼–ç¨‹åŠ©æ‰‹"},
                    {"role": "user", "content": example["instruction"]},
                    {"role": "assistant", "content": example["code"]}
                ]
                
                # åº”ç”¨èŠå¤©æ¨¡æ¿
                text = tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=False
                )
                
                # ç¼–ç 
                inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=2048).to(device)
                
                # å‰å‘ä¼ æ’­
                outputs = model(**inputs, labels=inputs["input_ids"])
                loss = outputs.loss
                
                # åå‘ä¼ æ’­
                loss.backward()
                total_loss += loss.item()
                successful_updates += 1
                
            except Exception as e:
                print(f"å¤„ç†ç¤ºä¾‹æ—¶å‡ºé”™: {str(e)}")
                continue
        
        # æ›´æ–°æ¨¡å‹å‚æ•°
        if successful_updates > 0:
            optimizer = torch.optim.AdamW(model.parameters(), lr=EVOLUTION_CONFIG["learning_rate"])
            optimizer.step()
            optimizer.zero_grad()
            
            avg_loss = total_loss / successful_updates if successful_updates > 0 else 0
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            os.makedirs(CHECKPOINT_DIR, exist_ok=True)
            checkpoint_path = f"{CHECKPOINT_DIR}/checkpoint_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pt"
            torch.save({
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': avg_loss,
                'examples_count': successful_updates
            }, checkpoint_path)
        else:
            avg_loss = 0
        
        model.eval()
        
        return f"âœ… å¾®è°ƒå®Œæˆï¼Œå¤„ç†äº†{successful_updates}ä¸ªç¤ºä¾‹ï¼Œå¹³å‡æŸå¤±: {avg_loss:.4f}"
        
    except Exception as e:
        model.eval()
        return f"å¾®è°ƒå¤±è´¥: {str(e)}"


def batch_self_evolution(problems: List[str], system_prompt: str = None) -> str:
    """
    æ‰¹é‡è‡ªæˆ‘æ¼”åŒ–æµç¨‹
    """
    if not problems:
        return "âŒ é”™è¯¯ï¼šæ²¡æœ‰æå–åˆ°æœ‰æ•ˆçš„ç¼–ç¨‹é—®é¢˜ã€‚è¯·ç¡®ä¿é—®é¢˜ç”¨å¼•å·æ‹¬èµ·æ¥ã€‚"
    
    total_problems = len(problems)
    batch_size = EVOLUTION_CONFIG["evolution_batch_size"]
    
    # åˆ›å»ºè¿›åº¦è·Ÿè¸ªå™¨
    tracker = create_progress_tracker(total_problems)
    
    report_lines = []
    report_lines.append("ğŸš€ å¼€å§‹æ‰¹é‡è‡ªæˆ‘æ¼”åŒ–æµç¨‹")
    report_lines.append(f"ğŸ“‹ æå–åˆ° {total_problems} ä¸ªç¼–ç¨‹é—®é¢˜")
    report_lines.append(f"ğŸ“¦ æ‰¹é‡å¤§å°: {batch_size}")
    report_lines.append("=" * 60)
    
    # æ˜¾ç¤ºæå–åˆ°çš„é—®é¢˜
    report_lines.append("ğŸ“ æå–åˆ°çš„é—®é¢˜ï¼š")
    for i, problem in enumerate(problems, 1):
        if len(problem) > 80:
            display_problem = problem[:77] + "..."
        else:
            display_problem = problem
        report_lines.append(f"  {i}. {display_problem}")
    
    report_lines.append("=" * 60)
    
    successful_examples = []
    
    # åˆ†æ‰¹å¤„ç†é—®é¢˜
    for i in range(0, total_problems, batch_size):
        batch = problems[i:i+batch_size]
        batch_num = i // batch_size + 1
        total_batches = (total_problems + batch_size - 1) // batch_size
        
        report_lines.append(f"\nğŸ“ å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches}")
        
        # å¹¶è¡Œå¤„ç†æ‰¹æ¬¡ä¸­çš„é—®é¢˜
        with ThreadPoolExecutor(max_workers=min(batch_size, 4)) as executor:
            future_to_problem = {
                executor.submit(process_single_problem, problem, system_prompt): problem 
                for problem in batch
            }
            
            for future in as_completed(future_to_problem):
                problem = future_to_problem[future]
                try:
                    success, result = future.result(timeout=120)
                    
                    # æ›´æ–°è¿›åº¦
                    if len(problem) > 50:
                        step_name = f"é—®é¢˜: {problem[:47]}..."
                    else:
                        step_name = f"é—®é¢˜: {problem}"
                    
                    progress_report, tracker = update_progress(
                        tracker, step_name, success, 
                        "æˆåŠŸ" if success else result.get("validation_result", "æœªçŸ¥é”™è¯¯")
                    )
                    
                    report_lines.append(progress_report)
                    
                    if success:
                        successful_examples.append(result)
                        report_lines.append(f"  âœ… å·²ä¿å­˜åˆ°: {result['saved_file']}")
                    else:
                        report_lines.append(f"  âŒ å¤±è´¥: {result.get('validation_result', 'æœªçŸ¥é”™è¯¯')[:80]}...")
                        
                except Exception as e:
                    progress_report, tracker = update_progress(tracker, "å¤„ç†å¼‚å¸¸", False, str(e))
                    report_lines.append(progress_report)
        
        report_lines.append("-" * 40)
    
    # å¾®è°ƒæ¨¡å‹
    if successful_examples:
        report_lines.append("\nğŸ¯ å¼€å§‹æ¨¡å‹å¾®è°ƒ...")
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        training_data = []
        for example in successful_examples:
            if example["success"]:
                training_data.append({
                    "instruction": example["problem"],
                    "code": example["generated_code"]
                })
        
        # æ‰§è¡Œå¾®è°ƒ
        fine_tune_result = fine_tune_on_examples(training_data)
        report_lines.append(fine_tune_result)
        
        # ä¿å­˜è®­ç»ƒç»Ÿè®¡
        stats_file = f"{TRAINING_DATA_DIR}/batch_evolution_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        stats = {
            "total_problems": total_problems,
            "successful": len(successful_examples),
            "failed": total_problems - len(successful_examples),
            "timestamp": datetime.now().isoformat(),
            "problems": problems,
            "examples_summary": [
                {
                    "problem": ex["problem"][:100] + "..." if len(ex["problem"]) > 100 else ex["problem"],
                    "success": ex["success"]
                }
                for ex in successful_examples[:10]
            ]
        }
        
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        report_lines.append(f"ğŸ“Š ç»Ÿè®¡æ•°æ®å·²ä¿å­˜åˆ°: {stats_file}")
    else:
        report_lines.append("\nâš ï¸ æ²¡æœ‰æˆåŠŸçš„ç¤ºä¾‹ï¼Œè·³è¿‡å¾®è°ƒ")
    
    # æœ€ç»ˆæŠ¥å‘Š
    report_lines.append("\n" + "=" * 60)
    report_lines.append("ğŸ‰ æ‰¹é‡è‡ªæˆ‘æ¼”åŒ–æµç¨‹å®Œæˆï¼")
    report_lines.append(f"âœ… æˆåŠŸå¤„ç†: {tracker['success']}/{total_problems}")
    report_lines.append(f"âŒ å¤±è´¥: {tracker['failed']}/{total_problems}")
    report_lines.append(f"â±ï¸ æ€»ç”¨æ—¶: {time.time() - tracker['start_time']:.1f}ç§’")
    
    if successful_examples:
        report_lines.append(f"ğŸ’¾ æ¨¡å‹å·²æ›´æ–°ï¼Œæ£€æŸ¥ç‚¹å·²ä¿å­˜")
    
    return "\n".join(report_lines)

def generate_code(prompt, system_prompt, max_tokens, temperature, top_p, enable_evolution=True):
    """ç”Ÿæˆä»£ç çš„ä¸»å‡½æ•°"""
    model, tokenizer, device = get_model()
    
    if model is None or tokenizer is None:
        return "é”™è¯¯ï¼šæ¨¡å‹å°šæœªåŠ è½½ï¼Œè¯·å…ˆç‚¹å‡»'åŠ è½½æ¨¡å‹'æŒ‰é’®ã€‚", ""
    
    if not prompt or prompt.strip() == "":
        return "é”™è¯¯ï¼šè¯·è¾“å…¥ä»£ç ç”Ÿæˆæç¤ºã€‚", ""
    
    # æ£€æµ‹æ˜¯å¦è§¦å‘è‡ªæˆ‘æ¼”åŒ–æ¨¡å¼
    should_evolve, extracted_problems = detect_evolution_mode(prompt)
    
    if should_evolve and enable_evolution:
        # è¿›å…¥è‡ªæˆ‘æ¼”åŒ–åˆ†æ”¯
        if extracted_problems:
            # æ‰¹é‡å¤„ç†æå–åˆ°çš„é—®é¢˜
            evolution_status = batch_self_evolution(extracted_problems, system_prompt)
            return evolution_status, ""
        else:
            # æ²¡æœ‰æå–åˆ°é—®é¢˜ï¼Œå¯èƒ½æ˜¯å•é—®é¢˜è‡ªæˆ‘æ¼”åŒ–
            try:
                # æå–å•ä¸ªé—®é¢˜ï¼ˆç§»é™¤æ¼”åŒ–å…³é”®è¯ï¼‰
                clean_prompt = prompt
                for keyword in EVOLUTION_CONFIG["evolution_keywords"]:
                    clean_prompt = clean_prompt.replace(keyword, "")
                clean_prompt = clean_prompt.strip()
                
                if clean_prompt:
                    success, result = process_single_problem(clean_prompt, system_prompt)
                    
                    if success:
                        status = f"âœ… å•é—®é¢˜è‡ªæˆ‘æ¼”åŒ–å®Œæˆï¼\n"
                        status += f"ğŸ“ å·²ä¿å­˜è®­ç»ƒæ•°æ®åˆ°: {result['saved_file']}\n"
                        
                        # å¾®è°ƒæ¨¡å‹
                        fine_tune_result = fine_tune_on_examples([{
                            "instruction": result["problem"],
                            "code": result["generated_code"]
                        }])
                        status += f"ğŸ¯ {fine_tune_result}"
                        
                        return status, result["generated_code"]
                    else:
                        return f"âŒ è‡ªæˆ‘æ¼”åŒ–å¤±è´¥:\n{result['validation_result']}", ""
                else:
                    return "âŒ é”™è¯¯ï¼šè¯·æä¾›è¦æ¼”åŒ–çš„å…·ä½“é—®é¢˜ã€‚", ""
                    
            except Exception as e:
                return f"è‡ªæˆ‘æ¼”åŒ–æ—¶å‡ºé”™ï¼š{str(e)}", ""
    else:
        # æ­£å¸¸ä»£ç ç”Ÿæˆåˆ†æ”¯
        try:
            messages = [
                {"role": "system", "content": system_prompt if system_prompt else "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¼–ç¨‹åŠ©æ‰‹ï¼Œæ“…é•¿ç¼–å†™å’Œè§£é‡Šä»£ç ã€‚"},
                {"role": "user", "content": prompt},
            ]
            
            text = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            model_inputs = tokenizer([text], return_tensors="pt").to(device)
            
            with torch.no_grad():
                generated_ids = model.generate(
                    **model_inputs,
                    max_new_tokens=int(max_tokens),
                    temperature=float(temperature),
                    top_p=float(top_p),
                    do_sample=True
                )
            
            generated_ids = [
                output_ids[len(input_ids):] 
                for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
            ]
            
            response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
            return "âœ… ä»£ç ç”Ÿæˆå®Œæˆ", response
            
        except Exception as e:
            return f"ç”Ÿæˆä»£ç æ—¶å‡ºé”™ï¼š{str(e)}", ""