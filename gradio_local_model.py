# ä½¿ç”¨æœ¬åœ° Qwen2.5-Coder-1.5B æ¨¡å‹çš„ Gradio ç•Œé¢
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer
from transformers import DataCollatorForLanguageModeling
import torch
import gradio as gr
import os
import json
import re
import traceback
import requests
import subprocess
import tempfile
from typing import Dict, List, Tuple, Optional
try:
    from datasets import Dataset
except ImportError:
    # å¦‚æœdatasetsåº“æœªå®‰è£…ï¼Œä½¿ç”¨ç®€å•çš„åˆ—è¡¨
    Dataset = None

# å…¨å±€å˜é‡å­˜å‚¨æ¨¡å‹å’Œåˆ†è¯å™¨
model = None
tokenizer = None
device = None

# è‡ªæˆ‘æ¼”åŒ–ç›¸å…³é…ç½®
SELF_EVOLUTION_KEYWORD = "è‡ªæˆ‘æ¼”åŒ–"
QWEN_32B_API_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions"  # Qwen APIåœ°å€
QWEN_14B_API_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions"  # Coder-14B APIåœ°å€
self_evolution_data = []  # å­˜å‚¨(instruct, code)äºŒå…ƒç»„
qwen_32b_api_key = "sk-1d1d9ecf1f1b446588871b3e6d5d3a30"
qwen_14b_api_key = "sk-4b834eacde4740479e7c66dbb8ebd46f"

# è®¾ç½®æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆè¯·æ ¹æ®ä½ çš„å®é™…è·¯å¾„ä¿®æ”¹ï¼‰
DEFAULT_MODEL_PATH = "./models/Qwen2.5-Coder-0.5B-instruct"

def load_model(model_path=None):
    """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
    global model, tokenizer, device
    
    # ä½¿ç”¨é»˜è®¤è·¯å¾„æˆ–ç”¨æˆ·æä¾›çš„è·¯å¾„
    if model_path is None or model_path.strip() == "":
        model_path = DEFAULT_MODEL_PATH
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
    if not os.path.exists(model_path):
        return f"é”™è¯¯ï¼šæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}\nè¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®ã€‚"
    
    try:
        print(f"æ­£åœ¨ä»æœ¬åœ°è·¯å¾„åŠ è½½æ¨¡å‹: {model_path}")
        print("æ­£åœ¨åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")
        
        # ä»æœ¬åœ°è·¯å¾„åŠ è½½åˆ†è¯å™¨
        tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)
        
        # ç¡®å®šè®¾å¤‡
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")
        
        # ä»æœ¬åœ°è·¯å¾„åŠ è½½æ¨¡å‹
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            local_files_only=True,  # åªä½¿ç”¨æœ¬åœ°æ–‡ä»¶ï¼Œä¸ä»ç½‘ç»œä¸‹è½½
            dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        model = model.to(device)
        model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        print("æ¨¡å‹åŠ è½½å®Œæˆï¼")
        return f"æ¨¡å‹åŠ è½½å®Œæˆï¼\næ¨¡å‹è·¯å¾„: {model_path}\nä½¿ç”¨è®¾å¤‡: {device}"
        
    except Exception as e:
        return f"åŠ è½½æ¨¡å‹æ—¶å‡ºé”™ï¼š{str(e)}"

def call_qwen_api(api_url: str, api_key: str, messages: List[Dict], model_name: str, max_tokens: int = 2048):
    """è°ƒç”¨Qwen APIç”Ÿæˆä»£ç """
    try:
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        data = {
            "model": model_name,
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": 0.7
        }
        response = requests.post(api_url, headers=headers, json=data, timeout=60)
        response.raise_for_status()
        result = response.json()
        return result["choices"][0]["message"]["content"]
    except Exception as e:
        raise Exception(f"APIè°ƒç”¨å¤±è´¥: {str(e)}")

def check_code_syntax(code: str) -> Tuple[bool, str]:
    """æ£€æŸ¥ä»£ç è¯­æ³•æ˜¯å¦æ­£ç¡®"""
    try:
        # ä½¿ç”¨Pythonç¼–è¯‘æ£€æŸ¥è¯­æ³•
        compile(code, '<string>', 'exec')
        return True, "è¯­æ³•æ£€æŸ¥é€šè¿‡"
    except SyntaxError as e:
        return False, f"è¯­æ³•é”™è¯¯: {str(e)}"
    except Exception as e:
        return False, f"ç¼–è¯‘é”™è¯¯: {str(e)}"

def check_code_logic(api_url: str, api_key: str, instruct: str, code: str) -> Tuple[bool, str]:
    """ä½¿ç”¨Coder-14Båˆ¤æ–­ä»£ç æ˜¯å¦ç¬¦åˆinstructé€»è¾‘"""
    try:
        messages = [
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä¸ªä»£ç å®¡æŸ¥ä¸“å®¶ï¼Œéœ€è¦åˆ¤æ–­ç”Ÿæˆçš„ä»£ç æ˜¯å¦ç¬¦åˆç”¨æˆ·çš„éœ€æ±‚ã€‚è¯·ä»”ç»†åˆ†æä»£ç é€»è¾‘ï¼Œåˆ¤æ–­ä»£ç æ˜¯å¦æ­£ç¡®å®ç°äº†ç”¨æˆ·çš„è¦æ±‚ã€‚åªå›ç­”'æ˜¯'æˆ–'å¦'ï¼Œç„¶åç®€è¦è¯´æ˜åŸå› ã€‚"
            },
            {
                "role": "user",
                "content": f"ç”¨æˆ·éœ€æ±‚ï¼š{instruct}\n\nç”Ÿæˆçš„ä»£ç ï¼š\n```python\n{code}\n```\n\nè¯·åˆ¤æ–­è¿™æ®µä»£ç æ˜¯å¦ç¬¦åˆç”¨æˆ·éœ€æ±‚ï¼Ÿ"
            }
        ]
        
        response_text = call_qwen_api(api_url, api_key, messages, "qwen2.5-coder-14b-instruct", max_tokens=200)
        
        # åˆ¤æ–­å“åº”ä¸­æ˜¯å¦åŒ…å«"æ˜¯"æˆ–"ç¬¦åˆ"ç­‰è‚¯å®šè¯æ±‡
        positive_keywords = ["æ˜¯", "ç¬¦åˆ", "æ­£ç¡®", "æ»¡è¶³", "å¯ä»¥", "èƒ½å¤Ÿ"]
        is_valid = any(keyword in response_text for keyword in positive_keywords)
        
        return is_valid, response_text
    except Exception as e:
        return False, f"é€»è¾‘æ£€æŸ¥å¤±è´¥: {str(e)}"

def self_evolution_process(instruct: str, qwen_32b_api_key: str, coder_14b_api_key: str, 
                          qwen_32b_url: str, coder_14b_url: str) -> str:
    """è‡ªæˆ‘æ¼”åŒ–åˆ†æ”¯ï¼šç”Ÿæˆä»£ç ã€éªŒè¯ã€æ”¶é›†æ•°æ®"""
    global self_evolution_data
    
    try:
        # æ­¥éª¤1: ä½¿ç”¨qwen2.5-coder-32b-instructç”Ÿæˆä»£ç 
        messages_32b = [
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¼–ç¨‹åŠ©æ‰‹ï¼Œæ“…é•¿ç¼–å†™å’Œè§£é‡Šä»£ç ã€‚è¯·æ ¹æ®ç”¨æˆ·éœ€æ±‚ç”Ÿæˆå®Œæ•´ã€æ­£ç¡®ã€å¯è¿è¡Œçš„Pythonä»£ç ã€‚"
            },
            {
                "role": "user",
                "content": instruct
            }
        ]
        
        generated_code = call_qwen_api(qwen_32b_url, qwen_32b_api_key, messages_32b, "qwen2.5-coder-32b-instruct")
        
        # æ­¥éª¤2: æ£€æŸ¥ä»£ç è¯­æ³•
        syntax_ok, syntax_msg = check_code_syntax(generated_code)
        if not syntax_ok:
            return f"ä»£ç è¯­æ³•æ£€æŸ¥å¤±è´¥\n{syntax_msg}\n\nç”Ÿæˆçš„ä»£ç ï¼š\n```python\n{generated_code}\n```"
        
        # æ­¥éª¤3: ä½¿ç”¨Coder-14Bæ£€æŸ¥ä»£ç é€»è¾‘
        logic_ok, logic_msg = check_code_logic(coder_14b_url, coder_14b_api_key, instruct, generated_code)
        if not logic_ok:
            return f"ä»£ç é€»è¾‘æ£€æŸ¥å¤±è´¥\n{logic_msg}\n\nç”Ÿæˆçš„ä»£ç ï¼š\n```python\n{generated_code}\n```"
        
        # æ­¥éª¤4: ä»£ç é€šè¿‡éªŒè¯ï¼Œæ·»åŠ åˆ°è®­ç»ƒæ•°æ®
        self_evolution_data.append({
            "instruct": instruct,
            "code": generated_code
        })
        
        return f"""è‡ªæˆ‘æ¼”åŒ–æ•°æ®æ”¶é›†æˆåŠŸï¼

**ç”¨æˆ·éœ€æ±‚**: {instruct}

**ç”Ÿæˆçš„ä»£ç **:
```python
{generated_code}
```

**éªŒè¯ç»“æœ**:
- è¯­æ³•æ£€æŸ¥: é€šè¿‡
- é€»è¾‘æ£€æŸ¥: é€šè¿‡

**å½“å‰æ•°æ®é›†å¤§å°**: {len(self_evolution_data)} æ¡

æ³¨æ„ï¼šè‡ªæˆ‘æ¼”åŒ–æ¨¡å¼åªæ”¶é›†æ•°æ®ç”¨äºå¾®è°ƒï¼Œä¸ç›´æ¥ç”Ÿæˆä»£ç ã€‚è¯·ä½¿ç”¨å¾®è°ƒåŠŸèƒ½è®­ç»ƒæ¨¡å‹ã€‚"""
        
    except Exception as e:
        return f"è‡ªæˆ‘æ¼”åŒ–è¿‡ç¨‹å‡ºé”™ï¼š{str(e)}\n{traceback.format_exc()}"

def fine_tune_model(output_dir: str = "./fine_tuned_model", num_epochs: int = 3):
    """ä½¿ç”¨æ”¶é›†çš„æ•°æ®å¾®è°ƒ1.5Bæ¨¡å‹"""
    global model, tokenizer, self_evolution_data
    
    if model is None or tokenizer is None:
        return "é”™è¯¯ï¼šæ¨¡å‹å°šæœªåŠ è½½ï¼Œè¯·å…ˆç‚¹å‡»'åŠ è½½æ¨¡å‹'æŒ‰é’®ã€‚"
    
    if len(self_evolution_data) == 0:
        return "é”™è¯¯ï¼šæ²¡æœ‰æ”¶é›†åˆ°è®­ç»ƒæ•°æ®ã€‚è¯·å…ˆä½¿ç”¨è‡ªæˆ‘æ¼”åŒ–åŠŸèƒ½æ”¶é›†æ•°æ®ã€‚"
    
    try:
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        def format_instruction(example):
            instruct = example["instruct"]
            code = example["code"]
            # æ ¼å¼åŒ–è®­ç»ƒæ•°æ®ï¼Œä½¿ç”¨èŠå¤©æ¨¡æ¿æ ¼å¼
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¼–ç¨‹åŠ©æ‰‹ï¼Œæ“…é•¿ç¼–å†™å’Œè§£é‡Šä»£ç ã€‚"},
                {"role": "user", "content": instruct},
                {"role": "assistant", "content": code}
            ]
            text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
            return {"text": text}
        
        # åˆ›å»ºæ•°æ®é›†
        formatted_data = [format_instruction(item) for item in self_evolution_data]
        
        if Dataset is not None:
            dataset = Dataset.from_list(formatted_data)
            
            # å¯¹æ•°æ®è¿›è¡Œtokenize
            def tokenize_function(examples):
                return tokenizer(
                    examples["text"],
                    truncation=True,
                    padding="max_length",
                    max_length=1024,
                    return_tensors="pt"
                )
            
            tokenized_dataset = dataset.map(tokenize_function, batched=True)
        else:
            # å¦‚æœæ²¡æœ‰datasetsåº“ï¼Œä½¿ç”¨ç®€å•æ–¹å¼
            tokenized_texts = []
            for item in formatted_data:
                tokens = tokenizer(
                    item["text"],
                    truncation=True,
                    padding="max_length",
                    max_length=1024,
                    return_tensors="pt"
                )
                tokenized_texts.append(tokens)
            # åˆ›å»ºä¸€ä¸ªç®€å•çš„æ•°æ®é›†åŒ…è£…
            class SimpleDataset:
                def __init__(self, data):
                    self.data = data
                def __len__(self):
                    return len(self.data)
                def __getitem__(self, idx):
                    return self.data[idx]
            tokenized_dataset = SimpleDataset(tokenized_texts)
        
        # è®¾ç½®è®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=num_epochs,
            per_device_train_batch_size=1,
            gradient_accumulation_steps=4,
            warmup_steps=10,
            logging_steps=10,
            save_steps=50,
            evaluation_strategy="no",
            save_total_limit=2,
            load_best_model_at_end=False,
            push_to_hub=False,
            report_to="none",  # ç¦ç”¨wandbç­‰
        )
        
        # åˆ›å»ºæ•°æ®æ•´ç†å™¨
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=tokenizer,
            mlm=False,
        )
        
        # åˆ›å»ºTrainer
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_dataset,
            data_collator=data_collator,
        )
        
        # å¼€å§‹è®­ç»ƒ
        trainer.train()
        
        # ä¿å­˜æ¨¡å‹
        trainer.save_model()
        tokenizer.save_pretrained(output_dir)
        
        return f"""æ¨¡å‹å¾®è°ƒå®Œæˆï¼

**è®­ç»ƒæ•°æ®é‡**: {len(self_evolution_data)} æ¡
**è®­ç»ƒè½®æ•°**: {num_epochs}
**æ¨¡å‹ä¿å­˜è·¯å¾„**: {output_dir}

å¾®è°ƒåçš„æ¨¡å‹å·²ä¿å­˜ï¼Œå¯ä»¥é‡æ–°åŠ è½½ä½¿ç”¨ã€‚"""
        
    except Exception as e:
        return f"å¾®è°ƒè¿‡ç¨‹å‡ºé”™ï¼š{str(e)}\n\n```\n{traceback.format_exc()}\n```"

def generate_code(prompt, system_prompt, max_tokens, temperature, top_p, 
                 qwen_32b_api_key, coder_14b_api_key, qwen_32b_url, coder_14b_url):
    """ç”Ÿæˆä»£ç çš„å‡½æ•°ï¼ˆæ”¯æŒè‡ªæˆ‘æ¼”åŒ–åˆ†æ”¯ï¼‰"""
    if model is None or tokenizer is None:
        return "é”™è¯¯ï¼šæ¨¡å‹å°šæœªåŠ è½½ï¼Œè¯·å…ˆç‚¹å‡»'åŠ è½½æ¨¡å‹'æŒ‰é’®ã€‚"
    
    if not prompt or prompt.strip() == "":
        return "é”™è¯¯ï¼šè¯·è¾“å…¥ä»£ç ç”Ÿæˆæç¤ºã€‚"
    
    # åˆ¤æ–­æ˜¯å¦è¿›å…¥è‡ªæˆ‘æ¼”åŒ–åˆ†æ”¯
    if SELF_EVOLUTION_KEYWORD in prompt:
        # è‡ªæˆ‘æ¼”åŒ–åˆ†æ”¯ï¼šåªæ”¶é›†æ•°æ®ï¼Œä¸ç”Ÿæˆä»£ç 
        if not qwen_32b_api_key or not coder_14b_api_key:
            return "é”™è¯¯ï¼šè‡ªæˆ‘æ¼”åŒ–æ¨¡å¼éœ€è¦é…ç½®APIå¯†é’¥ã€‚è¯·åœ¨è®¾ç½®ä¸­å¡«å†™Qwen-32Bå’ŒCoder-14Bçš„APIå¯†é’¥ã€‚"
        
        return self_evolution_process(
            prompt.replace(SELF_EVOLUTION_KEYWORD, "").strip(),  # ç§»é™¤å…³é”®è¯
            qwen_32b_api_key,
            coder_14b_api_key,
            qwen_32b_url if qwen_32b_url else QWEN_32B_API_URL,
            coder_14b_url if coder_14b_url else QWEN_14B_API_URL
        )
    
    # æ™®é€šä»£ç ç”Ÿæˆåˆ†æ”¯
    try:
        # å‡†å¤‡å¯¹è¯æ¶ˆæ¯
        messages = [
            {"role": "system", "content": system_prompt if system_prompt else "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¼–ç¨‹åŠ©æ‰‹ï¼Œæ“…é•¿ç¼–å†™å’Œè§£é‡Šä»£ç ã€‚"},
            {"role": "user", "content": prompt},
        ]
        
        # åº”ç”¨èŠå¤©æ¨¡æ¿
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        # å°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥
        model_inputs = tokenizer([text], return_tensors="pt").to(device)
        
        # ç”Ÿæˆä»£ç 
        with torch.no_grad():
            generated_ids = model.generate(
                **model_inputs,
                max_new_tokens=int(max_tokens),
                temperature=float(temperature),
                top_p=float(top_p),
                do_sample=True
            )
        
        # æå–ç”Ÿæˆçš„æ–‡æœ¬ï¼ˆå»æ‰è¾“å…¥éƒ¨åˆ†ï¼‰
        generated_ids = [
            output_ids[len(input_ids):] 
            for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
        ]
        
        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        return response
        
    except Exception as e:
        return f"ç”Ÿæˆä»£ç æ—¶å‡ºé”™ï¼š{str(e)}"

def extract_function_code(generated_text: str, entry_point: str) -> str:
    """ä»ç”Ÿæˆçš„æ–‡æœ¬ä¸­æå–å‡½æ•°ä»£ç """
    # æ¸…ç†ç”Ÿæˆçš„æ–‡æœ¬
    text = generated_text.strip()
    
    # æ–¹å¼1: æŸ¥æ‰¾å®Œæ•´çš„å‡½æ•°å®šä¹‰ï¼ˆåŒ…æ‹¬å‡½æ•°ç­¾åï¼‰
    pattern = rf'def\s+{re.escape(entry_point)}\s*\([^)]*\)\s*:.*?(?=\n\ndef\s+|\nclass\s+|$)'
    match = re.search(pattern, text, re.DOTALL)
    if match:
        return match.group(0).strip()
    
    # æ–¹å¼2: å¦‚æœåŒ…å«å‡½æ•°å®šä¹‰ï¼Œæå–å‡½æ•°ä½“ï¼ˆåŸºäºç¼©è¿›ï¼‰
    if f"def {entry_point}" in text:
        lines = text.split('\n')
        start_idx = -1
        for i, line in enumerate(lines):
            if f"def {entry_point}" in line:
                start_idx = i
                break
        
        if start_idx >= 0:
            result = [lines[start_idx]]  # åŒ…å«å‡½æ•°ç­¾å
            # è·å–å‡½æ•°ç­¾åçš„ç¼©è¿›çº§åˆ«
            base_indent = len(lines[start_idx]) - len(lines[start_idx].lstrip())
            
            # æå–å‡½æ•°ä½“
            for i in range(start_idx + 1, len(lines)):
                line = lines[i]
                if not line.strip():  # ç©ºè¡Œ
                    result.append(line)
                    continue
                
                current_indent = len(line) - len(line.lstrip())
                # å¦‚æœç¼©è¿›å°äºç­‰äºåŸºç¡€ç¼©è¿›ï¼Œè¯´æ˜å‡½æ•°ç»“æŸäº†
                if current_indent <= base_indent:
                    break
                result.append(line)
            
            return '\n'.join(result)
    
    # æ–¹å¼3: å°è¯•æŸ¥æ‰¾å‡½æ•°ä½“ï¼ˆå¯èƒ½æ²¡æœ‰å‡½æ•°ç­¾åï¼‰
    # æŸ¥æ‰¾ä»¥4ä¸ªç©ºæ ¼æˆ–tabå¼€å¤´çš„ä»£ç å—
    lines = text.split('\n')
    if lines and (lines[0].startswith('    ') or lines[0].startswith('\t')):
        # å¯èƒ½æ˜¯å‡½æ•°ä½“ï¼Œéœ€è¦æ·»åŠ å‡½æ•°ç­¾å
        # ä½†è¿™é‡Œæˆ‘ä»¬ä¸çŸ¥é“å‡½æ•°ç­¾åï¼Œæ‰€ä»¥ç›´æ¥è¿”å›
        return '\n'.join(lines)
    
    # æ–¹å¼4: å¦‚æœéƒ½æ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å›åŸå§‹æ–‡æœ¬ï¼ˆå¯èƒ½æ˜¯å®Œæ•´çš„å‡½æ•°ä½“ï¼‰
    return text

def evaluate_model(max_tasks: int = None, max_tokens: int = 512, temperature: float = 0.7, top_p: float = 0.9):
    """è¯„ä¼°æ¨¡å‹åœ¨ HumanEval æ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼ˆç”Ÿæˆå™¨å‡½æ•°ï¼Œæ”¯æŒæµå¼è¾“å‡ºï¼‰"""
    if model is None or tokenizer is None:
        yield "é”™è¯¯ï¼šæ¨¡å‹å°šæœªåŠ è½½ï¼Œè¯·å…ˆç‚¹å‡»'åŠ è½½æ¨¡å‹'æŒ‰é’®ã€‚"
        return
    
    dataset_path = "./datasets/human-eval-v2-20210705.jsonl"
    if not os.path.exists(dataset_path):
        yield f"é”™è¯¯ï¼šæ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {dataset_path}"
        return
    
    try:
        # è¯»å–æ•°æ®é›†
        tasks = []
        with open(dataset_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    tasks.append(json.loads(line))
        
        if max_tasks:
            tasks = tasks[:max_tasks]
        
        total_tasks = len(tasks)
        passed_tasks = 0
        failed_tasks = []
        results = []
        
        system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¼–ç¨‹åŠ©æ‰‹ï¼Œæ“…é•¿ç¼–å†™å’Œè§£é‡Šä»£ç ã€‚è¯·æ ¹æ®ç»™å®šçš„å‡½æ•°ç­¾åå’Œæ–‡æ¡£å­—ç¬¦ä¸²ï¼Œå®ç°è¯¥å‡½æ•°ã€‚"
        
        yield f"å¼€å§‹è¯„ä¼° {total_tasks} ä¸ªä»»åŠ¡...\n\n"
        
        for idx, task in enumerate(tasks):
            task_id = task['task_id']
            prompt = task['prompt']
            entry_point = task['entry_point']
            test_code = task['test']
            
            # ç”Ÿæˆä»£ç 
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt},
            ]
            
            text = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            model_inputs = tokenizer([text], return_tensors="pt").to(device)
            
            with torch.no_grad():
                generated_ids = model.generate(
                    **model_inputs,
                    max_new_tokens=max_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    do_sample=True
                )
            
            generated_ids = [
                output_ids[len(input_ids):] 
                for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
            ]
            
            generated_code = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
            
            # æå–å‡½æ•°ä»£ç 
            function_code = extract_function_code(generated_code, entry_point)
            
            # æ„å»ºå®Œæ•´ä»£ç ç”¨äºæµ‹è¯•
            full_code = prompt + function_code + "\n" + test_code
            
            # æ‰§è¡Œæµ‹è¯•
            try:
                # åˆ›å»ºå®‰å…¨çš„æ‰§è¡Œç¯å¢ƒ
                namespace = {}
                exec(full_code, namespace)
                
                # è¿è¡Œæµ‹è¯•
                check_func = namespace.get('check')
                candidate_func = namespace.get(entry_point)
                
                if check_func and candidate_func:
                    check_func(candidate_func)
                    passed_tasks += 1
                    results.append(f"âœ… {task_id}: é€šè¿‡")
                else:
                    failed_tasks.append(task_id)
                    results.append(f"âŒ {task_id}: å‡½æ•°æœªæ‰¾åˆ°ï¼ˆç”Ÿæˆçš„ä»£ç ä¸­å¯èƒ½æ²¡æœ‰æ­£ç¡®çš„å‡½æ•°å®šä¹‰ï¼‰")
                    
            except AssertionError as e:
                # æµ‹è¯•å¤±è´¥ï¼šç”Ÿæˆçš„å‡½æ•°æ²¡æœ‰é€šè¿‡æµ‹è¯•ç”¨ä¾‹
                # è¿™æ„å‘³ç€å‡½æ•°èƒ½å¤Ÿè¿è¡Œï¼Œä½†è¾“å‡ºç»“æœä¸æ­£ç¡®
                failed_tasks.append(task_id)
                error_msg = str(e)[:150] if str(e) else "æ–­è¨€å¤±è´¥"
                results.append(f"âŒ {task_id}: æµ‹è¯•å¤±è´¥ï¼ˆå‡½æ•°è¾“å‡ºä¸ç¬¦åˆé¢„æœŸï¼‰")
            except SyntaxError as e:
                # è¯­æ³•é”™è¯¯ï¼šç”Ÿæˆçš„ä»£ç æœ‰è¯­æ³•é—®é¢˜
                failed_tasks.append(task_id)
                error_msg = str(e)[:150]
                results.append(f"âŒ {task_id}: è¯­æ³•é”™è¯¯ - {error_msg}")
            except NameError as e:
                # åç§°é”™è¯¯ï¼šå¯èƒ½ç¼ºå°‘å¯¼å…¥æˆ–å‡½æ•°åé”™è¯¯
                failed_tasks.append(task_id)
                error_msg = str(e)[:150]
                results.append(f"âŒ {task_id}: åç§°é”™è¯¯ - {error_msg}")
            except Exception as e:
                # å…¶ä»–æ‰§è¡Œé”™è¯¯ï¼šè¿è¡Œæ—¶é”™è¯¯
                failed_tasks.append(task_id)
                error_type = type(e).__name__
                error_msg = str(e)[:150]  # æˆªæ–­é”™è¯¯ä¿¡æ¯
                results.append(f"âŒ {task_id}: {error_type} - {error_msg}")
            
            # å®æ—¶æ›´æ–°è¿›åº¦
            current_rate = (passed_tasks / (idx + 1) * 100) if (idx + 1) > 0 else 0
            progress_report = f"# æ¨¡å‹è¯„ä¼°è¿›åº¦\n\n"
            progress_report += f"**è¿›åº¦**: {idx + 1}/{total_tasks} ({((idx + 1)/total_tasks*100):.1f}%)\n\n"
            progress_report += f"**å½“å‰é€šè¿‡ç‡**: {current_rate:.2f}% ({passed_tasks}/{idx + 1})\n\n"
            progress_report += f"## è¯¦ç»†ç»“æœ\n\n"
            progress_report += "\n".join(results[-20:])  # æ˜¾ç¤ºæœ€è¿‘20ä¸ªç»“æœ
            
            yield progress_report
        
        # è®¡ç®—æœ€ç»ˆé€šè¿‡ç‡
        pass_rate = (passed_tasks / total_tasks * 100) if total_tasks > 0 else 0
        
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        final_report = f"""# æ¨¡å‹è¯„ä¼°æœ€ç»ˆæŠ¥å‘Š

## æ€»ä½“ç»“æœ
- **æ€»ä»»åŠ¡æ•°**: {total_tasks}
- **é€šè¿‡ä»»åŠ¡æ•°**: {passed_tasks}
- **å¤±è´¥ä»»åŠ¡æ•°**: {len(failed_tasks)}
- **é€šè¿‡ç‡**: {pass_rate:.2f}%

## è¯¦ç»†ç»“æœ
"""
        final_report += "\n".join(results)
        
        if failed_tasks:
            final_report += f"\n\n## å¤±è´¥çš„ä»»åŠ¡ID\n"
            final_report += ", ".join(failed_tasks[:20])  # åªæ˜¾ç¤ºå‰20ä¸ª
            if len(failed_tasks) > 20:
                final_report += f" ... (è¿˜æœ‰ {len(failed_tasks) - 20} ä¸ª)"
        
        yield final_report
        
    except Exception as e:
        yield f"è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºé”™ï¼š{str(e)}\n\n```\n{traceback.format_exc()}\n```"

# åˆ›å»º Gradio ç•Œé¢
with gr.Blocks(title="Qwen2.5-Coder æœ¬åœ°æ¨¡å‹ä»£ç ç”Ÿæˆå™¨", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# Qwen2.5-Coder æœ¬åœ°æ¨¡å‹ä»£ç ç”Ÿæˆå™¨")
    gr.Markdown("ä½¿ç”¨æœ¬åœ°ä¸‹è½½çš„ Qwen2.5-Coder-1.5B æ¨¡å‹ç”Ÿæˆä»£ç ã€‚")
    
    with gr.Row():
        with gr.Column():
            gr.Markdown("### æ¨¡å‹è®¾ç½®")
            model_path_input = gr.Textbox(
                label="æ¨¡å‹è·¯å¾„",
                value=DEFAULT_MODEL_PATH,
                placeholder="è¾“å…¥æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼Œä¾‹å¦‚: ./Qwen2.5-Coder-1.5B",
                lines=1
            )
            load_btn = gr.Button("åŠ è½½æ¨¡å‹", variant="primary", size="lg")
            load_status = gr.Textbox(label="æ¨¡å‹çŠ¶æ€", interactive=False, lines=3)
            
            gr.Markdown("### æ¨¡å‹è¯„ä¼°")
            with gr.Row():
                eval_max_tasks = gr.Number(
                    label="è¯„ä¼°ä»»åŠ¡æ•°é‡",
                    value=10,
                    minimum=1,
                    maximum=164,
                    step=1,
                    info="è¾“å…¥è¦è¯„ä¼°çš„ä»»åŠ¡æ•°é‡ï¼ˆ1-164ï¼‰ï¼Œå»ºè®®å…ˆç”¨å°‘é‡ä»»åŠ¡æµ‹è¯•ã€‚ç•™ç©ºæˆ–0è¡¨ç¤ºè¯„ä¼°å…¨éƒ¨ä»»åŠ¡ã€‚"
                )
                eval_all_check = gr.Checkbox(
                    label="è¯„ä¼°å…¨éƒ¨ä»»åŠ¡ï¼ˆ164ä¸ªï¼‰",
                    value=False,
                    info="å‹¾é€‰æ­¤é¡¹å°†è¯„ä¼°æ‰€æœ‰164ä¸ªä»»åŠ¡"
                )
            eval_btn = gr.Button("ğŸš€ å¼€å§‹è¯„ä¼°", variant="secondary", size="lg")
            eval_output = gr.Markdown(label="è¯„ä¼°ç»“æœ")
            
            # æ·»åŠ è¯„ä¼°è¯´æ˜
            with gr.Accordion("ğŸ“– è¯„ä¼°è¯´æ˜", open=False):
                gr.Markdown("""
                ### è¯„ä¼°ç»“æœè¯´æ˜
                
                **âœ… é€šè¿‡**: ç”Ÿæˆçš„å‡½æ•°é€šè¿‡äº†æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹
                
                **âŒ æµ‹è¯•å¤±è´¥**: 
                - å‡½æ•°èƒ½å¤Ÿæ­£å¸¸æ‰§è¡Œï¼Œä½†è¾“å‡ºç»“æœä¸ç¬¦åˆé¢„æœŸ
                - è¯´æ˜ç”Ÿæˆçš„ä»£ç é€»è¾‘æœ‰è¯¯
                - ä¾‹å¦‚ï¼šè¿”å›å€¼é”™è¯¯ã€è¾¹ç•Œæ¡ä»¶å¤„ç†ä¸å½“ç­‰
                
                **âŒ è¯­æ³•é”™è¯¯**: 
                - ç”Ÿæˆçš„ä»£ç å­˜åœ¨Pythonè¯­æ³•é—®é¢˜
                - ä¾‹å¦‚ï¼šç¼ºå°‘å†’å·ã€æ‹¬å·ä¸åŒ¹é…ã€ç¼©è¿›é”™è¯¯ç­‰
                
                **âŒ åç§°é”™è¯¯**: 
                - ä»£ç ä¸­ä½¿ç”¨äº†æœªå®šä¹‰çš„å˜é‡æˆ–å‡½æ•°
                - å¯èƒ½ç¼ºå°‘å¿…è¦çš„å¯¼å…¥è¯­å¥
                
                **âŒ å‡½æ•°æœªæ‰¾åˆ°**: 
                - ç”Ÿæˆçš„ä»£ç ä¸­æ²¡æœ‰æ‰¾åˆ°ç›®æ ‡å‡½æ•°
                - å¯èƒ½æ˜¯å‡½æ•°åä¸åŒ¹é…æˆ–ä»£ç æ ¼å¼é—®é¢˜
                
                **âŒ å…¶ä»–æ‰§è¡Œé”™è¯¯**: 
                - è¿è¡Œæ—¶å‡ºç°çš„å…¶ä»–é”™è¯¯
                - ä¾‹å¦‚ï¼šç±»å‹é”™è¯¯ã€ç´¢å¼•è¶Šç•Œç­‰
                
                ### HumanEvalæ•°æ®é›†
                - åŒ…å«164ä¸ªPythonç¼–ç¨‹ä»»åŠ¡
                - æ¯ä¸ªä»»åŠ¡éƒ½æœ‰å¤šä¸ªæµ‹è¯•ç”¨ä¾‹
                - åªæœ‰é€šè¿‡æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹æ‰ç®—é€šè¿‡
                """)
            
            with gr.Accordion("é«˜çº§è®¾ç½®", open=False):
                system_prompt_input = gr.Textbox(
                    label="ç³»ç»Ÿæç¤ºè¯",
                    value="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¼–ç¨‹åŠ©æ‰‹ï¼Œæ“…é•¿ç¼–å†™å’Œè§£é‡Šä»£ç ã€‚",
                    lines=2,
                    placeholder="è¾“å…¥ç³»ç»Ÿæç¤ºè¯..."
                )
                max_tokens_input = gr.Slider(
                    label="æœ€å¤§ç”Ÿæˆtokenæ•°",
                    minimum=50,
                    maximum=2048,
                    value=512,
                    step=50
                )
                temperature_input = gr.Slider(
                    label="Temperature (åˆ›é€ æ€§)",
                    minimum=0.1,
                    maximum=2.0,
                    value=0.7,
                    step=0.1
                )
                top_p_input = gr.Slider(
                    label="Top-p (æ ¸é‡‡æ ·)",
                    minimum=0.1,
                    maximum=1.0,
                    value=0.9,
                    step=0.05
                )
            
            with gr.Accordion("è‡ªæˆ‘æ¼”åŒ–é…ç½®", open=False):
                gr.Markdown("### APIé…ç½®ï¼ˆè‡ªæˆ‘æ¼”åŒ–æ¨¡å¼éœ€è¦ï¼‰")
                qwen_32b_api_key = gr.Textbox(
                    label="qwen2.5-coder-32b-instruct API Key",
                    type="password",
                    placeholder="è¾“å…¥APIå¯†é’¥",
                    info="ç”¨äºç”Ÿæˆé«˜è´¨é‡ä»£ç "
                )
                qwen_32b_url = gr.Textbox(
                    label="qwen2.5-coder-32b-instruct API URL",
                    value=QWEN_32B_API_URL,
                    placeholder="APIåœ°å€",
                    info="Qwen-32Bæ¨¡å‹çš„APIåœ°å€"
                )
                qwen_14b_api_key = gr.Textbox(
                    label="qwen2.5-coder-14b-instruct API Key",
                    type="password",
                    placeholder="è¾“å…¥APIå¯†é’¥",
                    info="ç”¨äºéªŒè¯ä»£ç é€»è¾‘"
                )
                qwen_14b_url = gr.Textbox(
                    label="qwen2.5-coder-14b-instruct API URL",
                    value=QWEN_14B_API_URL,
                    placeholder="APIåœ°å€",
                    info="qwen2.5-coder-14b-instructæ¨¡å‹çš„APIåœ°å€"
                )
                gr.Markdown("""
                ### ä½¿ç”¨è¯´æ˜
                - åœ¨ä»£ç ç”Ÿæˆæç¤ºä¸­åŒ…å«"**è‡ªæˆ‘æ¼”åŒ–**"å…³é”®è¯å³å¯è¿›å…¥è‡ªæˆ‘æ¼”åŒ–æ¨¡å¼
                - è‡ªæˆ‘æ¼”åŒ–æ¨¡å¼ä¼šä½¿ç”¨32Bæ¨¡å‹ç”Ÿæˆä»£ç ï¼ŒéªŒè¯åæ”¶é›†è®­ç»ƒæ•°æ®
                - æ”¶é›†çš„æ•°æ®å¯ç”¨äºå¾®è°ƒ1.5Bæ¨¡å‹
                - è‡ªæˆ‘æ¼”åŒ–æ¨¡å¼**åªæ”¶é›†æ•°æ®ï¼Œä¸ç›´æ¥ç”Ÿæˆä»£ç **
                """)
            
            with gr.Accordion("æ¨¡å‹å¾®è°ƒ", open=False):
                gr.Markdown("### ä½¿ç”¨æ”¶é›†çš„æ•°æ®å¾®è°ƒæ¨¡å‹")
                fine_tune_output_dir = gr.Textbox(
                    label="æ¨¡å‹ä¿å­˜è·¯å¾„",
                    value="./fine_tuned_model",
                    placeholder="./fine_tuned_model",
                    info="å¾®è°ƒåæ¨¡å‹çš„ä¿å­˜è·¯å¾„"
                )
                fine_tune_epochs = gr.Slider(
                    label="è®­ç»ƒè½®æ•°",
                    minimum=1,
                    maximum=10,
                    value=3,
                    step=1,
                    info="å¾®è°ƒçš„è®­ç»ƒè½®æ•°"
                )
                fine_tune_btn = gr.Button("ğŸš€ å¼€å§‹å¾®è°ƒ", variant="secondary", size="lg")
                fine_tune_output = gr.Markdown(label="å¾®è°ƒç»“æœ")
                fine_tune_status = gr.Textbox(
                    label="å½“å‰æ•°æ®é›†çŠ¶æ€",
                    value=f"å·²æ”¶é›†æ•°æ®: 0 æ¡",
                    interactive=False
                )
        
        with gr.Column():
            gr.Markdown("### ä»£ç ç”Ÿæˆ")
            prompt_input = gr.Textbox(
                label="ä»£ç ç”Ÿæˆæç¤º",
                placeholder="ä¾‹å¦‚ï¼šè¯·ç”¨Pythonç¼–å†™ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•ã€‚",
                lines=5
            )
            generate_btn = gr.Button("âœ¨ ç”Ÿæˆä»£ç ", variant="primary", size="lg")
            output = gr.Code(
                label="ç”Ÿæˆçš„ä»£ç ",
                language="python",
                lines=20
            )
    
    # ç»‘å®šäº‹ä»¶
    load_btn.click(
        fn=load_model,
        inputs=model_path_input,
        outputs=load_status
    )
    
    def update_data_status():
        """æ›´æ–°æ•°æ®é›†çŠ¶æ€"""
        return f"å·²æ”¶é›†æ•°æ®: {len(self_evolution_data)} æ¡"
    
    def generate_code_with_status_update(prompt, system_prompt, max_tokens, temperature, top_p,
                                        qwen_32b_key, coder_14b_key, qwen_32b_url_val, coder_14b_url_val):
        """ç”Ÿæˆä»£ç å¹¶æ›´æ–°æ•°æ®é›†çŠ¶æ€"""
        result = generate_code(prompt, system_prompt, max_tokens, temperature, top_p,
                              qwen_32b_key, coder_14b_key, qwen_32b_url_val, coder_14b_url_val)
        status = update_data_status()
        return result, status
    
    generate_btn.click(
        fn=generate_code_with_status_update,
        inputs=[
            prompt_input, 
            system_prompt_input, 
            max_tokens_input, 
            temperature_input, 
            top_p_input,
            qwen_32b_api_key,
            qwen_14b_api_key,
            qwen_32b_url,
            qwen_14b_url
        ],
        outputs=[output, fine_tune_status]
    )
    
    def fine_tune_wrapper(output_dir, epochs):
        """å¾®è°ƒåŒ…è£…å‡½æ•°"""
        result = fine_tune_model(output_dir, int(epochs))
        status = update_data_status()
        return result, status
    
    fine_tune_btn.click(
        fn=fine_tune_wrapper,
        inputs=[fine_tune_output_dir, fine_tune_epochs],
        outputs=[fine_tune_output, fine_tune_status]
    )
    
    def evaluate_wrapper(max_tasks, eval_all, max_tokens, temperature, top_p):
        """è¯„ä¼°å‡½æ•°çš„åŒ…è£…å™¨ï¼Œæ”¯æŒæµå¼è¾“å‡º"""
        if eval_all:
            max_tasks_val = None  # è¯„ä¼°å…¨éƒ¨ä»»åŠ¡
        else:
            max_tasks_val = int(max_tasks) if max_tasks and max_tasks > 0 else None
        for result in evaluate_model(max_tasks_val, max_tokens, temperature, top_p):
            yield result
    
    eval_btn.click(
        fn=evaluate_wrapper,
        inputs=[eval_max_tasks, eval_all_check, max_tokens_input, temperature_input, top_p_input],
        outputs=eval_output
    )
    
    # ç¤ºä¾‹æç¤ºè¯
    gr.Examples(
        examples=[
            ["è¯·ç”¨Pythonç¼–å†™ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•ã€‚"],
            ["ç”¨Pythonå®ç°ä¸€ä¸ªç®€å•çš„HTTPæœåŠ¡å™¨ã€‚"],
            ["å†™ä¸€ä¸ªå‡½æ•°æ¥è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„ç¬¬né¡¹ã€‚"],
            ["ç”¨Pythonå®ç°ä¸€ä¸ªç®€å•çš„è®¡ç®—å™¨ç±»ã€‚"],
            ["è‡ªæˆ‘æ¼”åŒ– è¯·ç”¨Pythonç¼–å†™ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•ã€‚"],
            ["è‡ªæˆ‘æ¼”åŒ– å®ç°ä¸€ä¸ªç®€å•çš„HTTPæœåŠ¡å™¨ã€‚"],
        ],
        inputs=prompt_input
    )

if __name__ == "__main__":
    # å¯åŠ¨ Gradio ç•Œé¢
    demo.launch(share=False, server_name="0.0.0.0", server_port=7860)

