# ä½¿ç”¨æœ¬åœ° Qwen2.5-Coder-1.5B æ¨¡å‹çš„ Gradio ç•Œé¢ï¼Œå¸¦æ‰¹é‡è‡ªæˆ‘æ¼”åŒ–åŠŸèƒ½
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import gradio as gr
import os
import json
import subprocess
import tempfile
import requests
import re
import time
from datetime import datetime
from typing import Dict, List, Tuple, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed

# ====== å…¨å±€å˜é‡ ======
model = None
tokenizer = None
device = None

# æ¨¡å‹è·¯å¾„
DEFAULT_MODEL_PATH = "./models/Qwen2.5-Coder-0.5B-Instruct"

# APIé…ç½®ï¼ˆ32Bå’Œ14Bæ¨¡å‹ï¼‰
API_CONFIG = {
    "qwen_32b_api_url": "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions",
    "qwen_14b_api_url": "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions",
    "api_key": "sk-0de8170042f14c87b88adb94a9c3d115",
}

# è‡ªæˆ‘æ¼”åŒ–é…ç½®
EVOLUTION_CONFIG = {
    "enable_self_evolution": True,
    "evolution_keywords": ["è‡ªæˆ‘æ¼”åŒ–", "è‡ªæˆ‘è¿›åŒ–", "self-evolve", "self-evolution"],
    "max_fine_tune_steps": 100,
    "learning_rate": 5e-5,
    "evolution_batch_size": 3,
}

# å­˜å‚¨è®­ç»ƒæ•°æ®çš„ç›®å½•
TRAINING_DATA_DIR = "./evolution_training_data"

# ====== æ–‡æœ¬å¤„ç†å‡½æ•° ======
def extract_problems_from_text(text: str) -> List[str]:
    """
    ä»æ–‡æœ¬ä¸­æå–å¼•å·å†…çš„ç¼–ç¨‹é—®é¢˜
    
    æ”¯æŒæ ¼å¼ï¼š
    1. å•å¼•å·æˆ–åŒå¼•å·ï¼š'problem' æˆ– "problem"
    2. å¤šè¡Œè¾“å…¥ï¼Œæ¯è¡Œä¸€ä¸ªé—®é¢˜
    3. åŒ…å«"è‡ªæˆ‘æ¼”åŒ–"å…³é”®è¯çš„æç¤ºæ–‡æœ¬
    """
    # ç§»é™¤"è‡ªæˆ‘æ¼”åŒ–"å…³é”®è¯å’Œå¯èƒ½çš„æç¤ºæ–‡æœ¬
    clean_text = text.lower()
    for keyword in ["è‡ªæˆ‘æ¼”åŒ–", "self-evolve", "self-evolution", "è¯·è‡ªæˆ‘æ¼”åŒ–", "è¯·è¿›åŒ–"]:
        clean_text = clean_text.replace(keyword.lower(), "")
    
    # æå–æ‰€æœ‰å¼•å·å†…çš„å†…å®¹
    # åŒ¹é…åŒå¼•å·
    double_quote_pattern = r'"([^"]*)"'
    # åŒ¹é…å•å¼•å·
    single_quote_pattern = r"'([^']*)'"
    
    problems = []
    
    # æå–åŒå¼•å·å†…å®¹
    for match in re.findall(double_quote_pattern, text):
        if match.strip() and len(match.strip()) > 10:  # ç¡®ä¿ä¸æ˜¯ç©ºå­—ç¬¦ä¸²ä¸”æœ‰ä¸€å®šé•¿åº¦
            problems.append(match.strip())
    
    # æå–å•å¼•å·å†…å®¹
    for match in re.findall(single_quote_pattern, text):
        if match.strip() and len(match.strip()) > 10:
            problems.append(match.strip())
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¼•å·å†…å®¹ï¼Œå°è¯•æŒ‰è¡Œåˆ†å‰²
    if not problems:
        lines = text.split('\n')
        for line in lines:
            line = line.strip()
            # è·³è¿‡ç©ºè¡Œå’Œå¤ªçŸ­çš„è¡Œ
            if line and len(line) > 20 and not line.startswith('#'):
                # ç§»é™¤è¡Œå·ã€é¡¹ç›®ç¬¦å·ç­‰
                clean_line = re.sub(r'^\s*\d+[\.\)]?\s*', '', line)  # ç§»é™¤ "1. " æˆ– "1) "
                clean_line = re.sub(r'^\s*[â€¢\-*]\s*', '', clean_line)  # ç§»é™¤é¡¹ç›®ç¬¦å·
                clean_line = clean_line.strip()
                if clean_line and len(clean_line) > 20:
                    problems.append(clean_line)
    
    # å»é‡
    unique_problems = []
    seen = set()
    for problem in problems:
        problem_lower = problem.lower()
        if problem_lower not in seen:
            seen.add(problem_lower)
            unique_problems.append(problem)
    
    return unique_problems

def detect_evolution_mode(prompt: str) -> Tuple[bool, List[str]]:
    """
    æ£€æµ‹æ˜¯å¦è¿›å…¥è‡ªæˆ‘æ¼”åŒ–æ¨¡å¼ï¼Œå¹¶æå–é—®é¢˜
    è¿”å›: (æ˜¯å¦æ¼”åŒ–æ¨¡å¼, é—®é¢˜åˆ—è¡¨)
    """
    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ¼”åŒ–å…³é”®è¯
    should_evolve = False
    for keyword in EVOLUTION_CONFIG["evolution_keywords"]:
        if keyword.lower() in prompt.lower():
            should_evolve = True
            break
    
    if not should_evolve:
        return False, []
    
    # æå–é—®é¢˜
    problems = extract_problems_from_text(prompt)
    return True, problems

# ====== è¾…åŠ©å‡½æ•° ======
def create_progress_tracker(total_steps: int):
    """åˆ›å»ºè¿›åº¦è·Ÿè¸ªå™¨"""
    return {
        "total": total_steps,
        "current": 0,
        "success": 0,
        "failed": 0,
        "start_time": time.time(),
        "logs": []
    }

def update_progress(tracker: Dict, step_name: str, success: bool = True, message: str = ""):
    """æ›´æ–°è¿›åº¦è·Ÿè¸ªå™¨"""
    tracker["current"] += 1
    if success:
        tracker["success"] += 1
    else:
        tracker["failed"] += 1
    
    progress_percent = (tracker["current"] / tracker["total"]) * 100
    elapsed_time = time.time() - tracker["start_time"]
    
    log_entry = {
        "step": tracker["current"],
        "name": step_name,
        "success": success,
        "message": message,
        "progress": f"{progress_percent:.1f}%",
        "elapsed": f"{elapsed_time:.1f}s"
    }
    
    tracker["logs"].append(log_entry)
    
    # æ„å»ºçŠ¶æ€æŠ¥å‘Š
    report = f"ğŸ“Š è¿›åº¦: {progress_percent:.1f}% ({tracker['current']}/{tracker['total']})\n"
    report += f"âœ… æˆåŠŸ: {tracker['success']} | âŒ å¤±è´¥: {tracker['failed']}\n"
    report += f"â±ï¸ ç”¨æ—¶: {elapsed_time:.1f}ç§’\n"
    report += f"ğŸ“ å½“å‰æ­¥éª¤: {step_name}\n"
    if message:
        report += f"ğŸ’¬ {message[:100]}...\n" if len(message) > 100 else f"ğŸ’¬ {message}\n"
    
    return report, tracker

# ====== APIè°ƒç”¨å‡½æ•° ======
def call_qwen_api(api_url: str, prompt: str, model_name: str = "qwen2.5-coder-32b-instruct", 
                  max_tokens: int = 1024, temperature: float = 0.7, 
                  retries: int = 3) -> Tuple[bool, str]:
    """
    è°ƒç”¨Qwen APIç”Ÿæˆä»£ç 
    """
    headers = {
        "Authorization": f"Bearer {API_CONFIG['api_key']}",
        "Content-Type": "application/json"
    }
    
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¼–ç¨‹åŠ©æ‰‹ï¼Œè¯·ç”Ÿæˆé«˜è´¨é‡ã€å¯è¿è¡Œçš„Pythonä»£ç ã€‚"},
        {"role": "user", "content": prompt}
    ]
    
    payload = {
        "model": model_name,
        "messages": messages,
        "max_tokens": max_tokens,
        "temperature": temperature,
        "top_p": 0.9
    }
    
    for attempt in range(retries):
        try:
            response = requests.post(api_url, headers=headers, json=payload, timeout=60)
            response.raise_for_status()
            result = response.json()
            generated_code = result["choices"][0]["message"]["content"]
            
            # æå–ä»£ç å—ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            code_pattern = r"```(?:python)?\n?(.*?)```"
            matches = re.findall(code_pattern, generated_code, re.DOTALL)
            
            if matches:
                generated_code = matches[0].strip()
            
            return True, generated_code
        except requests.exceptions.RequestException as e:
            if attempt == retries - 1:
                return False, f"APIè°ƒç”¨å¤±è´¥ï¼ˆå°è¯•{retries}æ¬¡ï¼‰: {str(e)}"
            time.sleep(1)  # ç­‰å¾…1ç§’åé‡è¯•
        except Exception as e:
            return False, f"APIå¤„ç†å¤±è´¥: {str(e)}"
    
    return False, "æœªçŸ¥é”™è¯¯"

def validate_code_with_14b(instruct: str, code: str) -> Tuple[bool, str]:
    """
    ä½¿ç”¨14Bæ¨¡å‹éªŒè¯ä»£ç æ˜¯å¦ç¬¦åˆæŒ‡ä»¤é€»è¾‘
    """
    validation_prompt = f"""
    è¯·åˆ†æä»¥ä¸‹ä»£ç æ˜¯å¦ç¬¦åˆç”¨æˆ·æŒ‡ä»¤çš„é€»è¾‘è¦æ±‚ï¼š
    
    ç”¨æˆ·æŒ‡ä»¤ï¼š{instruct}
    
    ç”Ÿæˆçš„ä»£ç ï¼š
    ```python
    {code}
    ```
    
    è¯·ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢è¿›è¡Œåˆ¤æ–­ï¼š
    1. ä»£ç æ˜¯å¦å®Œæ•´å®ç°äº†æŒ‡ä»¤è¦æ±‚çš„åŠŸèƒ½
    2. ä»£ç é€»è¾‘æ˜¯å¦æ­£ç¡®
    3. æ˜¯å¦æœ‰æ˜æ˜¾çš„é€»è¾‘é”™è¯¯æˆ–ç¼ºå¤±
    
    è¯·ç”¨ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š
    [æ˜¯å¦é€šè¿‡]ï¼šæ˜¯/å¦
    [ç†ç”±]ï¼šç®€è¦è¯´æ˜ç†ç”±
    """
    
    success, response = call_qwen_api(
        API_CONFIG["qwen_14b_api_url"], 
        validation_prompt, 
        model_name="qwen2.5-coder-14b-instruct",
        max_tokens=256,
        temperature=0.3
    )
    
    if not success:
        return False, response
    
    # è§£æå“åº”
    if "[æ˜¯å¦é€šè¿‡]ï¼šæ˜¯" in response or "é€šè¿‡" in response and "å¦" not in response:
        return True, response
    else:
        return False, response

# ====== ä»£ç éªŒè¯å‡½æ•° ======
def check_code_syntax(code: str) -> Tuple[bool, str]:
    """
    æ£€æŸ¥Pythonä»£ç çš„è¯­æ³•é”™è¯¯
    """
    try:
        # æ·»åŠ å¿…è¦çš„å¯¼å…¥
        full_code = "import math\nimport re\nimport heapq\nimport numpy as np\n" + code
        
        # å°è¯•ç¼–è¯‘
        compile(full_code, '<string>', 'exec')
        return True, "è¯­æ³•æ£€æŸ¥é€šè¿‡"
    except SyntaxError as e:
        return False, f"è¯­æ³•é”™è¯¯: {str(e)}"
    except Exception as e:
        return False, f"ä»£ç æ£€æŸ¥é”™è¯¯: {str(e)}"

def run_simple_test(code: str, problem_type: str) -> Tuple[bool, str]:
    """
    è¿è¡Œç®€å•çš„æµ‹è¯•ç”¨ä¾‹
    """
    try:
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
            f.write(code)
            temp_file = f.name
        
        # æ ¹æ®é—®é¢˜ç±»å‹æ·»åŠ æµ‹è¯•
        test_code = ""
        if "minimum cost path" in problem_type.lower():
            test_code = """
if __name__ == "__main__":
    cost_matrix = [[1, 2, 3], [4, 8, 2], [1, 5, 3]]
    try:
        result = min_cost_path(cost_matrix, 2, 2)
        print(f"æµ‹è¯•é€šè¿‡ï¼Œæœ€å°æˆæœ¬: {result}")
    except Exception as e:
        print(f"æµ‹è¯•å¤±è´¥: {e}")
"""
        elif "similar elements" in problem_type.lower():
            test_code = """
if __name__ == "__main__":
    list1 = [(1, 2), (3, 4), (5, 6)]
    list2 = [(3, 4), (7, 8), (1, 2)]
    try:
        result = find_similar_elements(list1, list2)
        print(f"æµ‹è¯•é€šè¿‡ï¼Œç›¸ä¼¼å…ƒç´ : {result}")
    except Exception as e:
        print(f"æµ‹è¯•å¤±è´¥: {e}")
"""
        # æ·»åŠ æ›´å¤šæµ‹è¯•ç±»å‹...
        
        if test_code:
            with open(temp_file, 'a') as f:
                f.write(test_code)
            
            result = subprocess.run(
                ['python', temp_file],
                capture_output=True,
                text=True,
                timeout=5
            )
            
            os.unlink(temp_file)
            
            if result.returncode == 0:
                return True, f"æµ‹è¯•é€šè¿‡: {result.stdout.strip()}"
            else:
                return False, f"æµ‹è¯•å¤±è´¥: {result.stderr.strip()}"
        else:
            os.unlink(temp_file)
            return True, "æ— ç‰¹å®šæµ‹è¯•ï¼Œè·³è¿‡è¿è¡Œæµ‹è¯•"
            
    except Exception as e:
        if os.path.exists(temp_file):
            os.unlink(temp_file)
        return False, f"æµ‹è¯•æ‰§è¡Œé”™è¯¯: {str(e)}"

# ====== è®­ç»ƒæ•°æ®å¤„ç† ======
def save_training_example(instruct: str, code: str, validation_result: str):
    """
    ä¿å­˜è®­ç»ƒæ•°æ®åˆ°æ–‡ä»¶
    """
    os.makedirs(TRAINING_DATA_DIR, exist_ok=True)
    
    training_example = {
        "instruction": instruct,
        "code": code,
        "validation_result": validation_result,
        "timestamp": datetime.now().isoformat(),
        "metadata": {
            "source": "self_evolution",
            "problem_type": classify_problem(instruct)
        }
    }
    
    # ç”Ÿæˆæ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    problem_hash = hash(instruct) % 10000
    filename = f"{TRAINING_DATA_DIR}/example_{timestamp}_{problem_hash}.json"
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(training_example, f, ensure_ascii=False, indent=2)
    
    return filename

def classify_problem(instruct: str) -> str:
    """
    åˆ†ç±»é—®é¢˜ç±»å‹
    """
    keywords = {
        "path": ["minimum cost path", "cost matrix", "reach", "grid"],
        "search": ["find", "search", "identify", "check"],
        "sort": ["sort", "largest", "smallest", "order"],
        "string": ["string", "character", "words", "regex"],
        "math": ["prime", "number", "bit", "volume", "rotations"],
        "data_structure": ["heap", "matrix", "list", "tuple", "dictionary"]
    }
    
    instruct_lower = instruct.lower()
    for category, words in keywords.items():
        for word in words:
            if word in instruct_lower:
                return category
    
    return "general"

# ====== æ¨¡å‹è®­ç»ƒå‡½æ•° ======
def fine_tune_on_examples(examples: List[Dict]) -> str:
    """
    åœ¨å¤šä¸ªç¤ºä¾‹ä¸Šå¾®è°ƒæ¨¡å‹
    """
    global model, tokenizer
    
    if model is None or tokenizer is None:
        return "æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•è¿›è¡Œå¾®è°ƒ"
    
    if not examples:
        return "æ²¡æœ‰æœ‰æ•ˆçš„è®­ç»ƒç¤ºä¾‹"
    
    try:
        model.train()
        
        total_loss = 0
        successful_updates = 0
        
        for example in examples:
            try:
                # å‡†å¤‡è®­ç»ƒæ•°æ®
                messages = [
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¼–ç¨‹åŠ©æ‰‹"},
                    {"role": "user", "content": example["instruction"]},
                    {"role": "assistant", "content": example["code"]}
                ]
                
                # åº”ç”¨èŠå¤©æ¨¡æ¿
                text = tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=False
                )
                
                # ç¼–ç 
                inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=2048).to(device)
                
                # å‰å‘ä¼ æ’­
                outputs = model(**inputs, labels=inputs["input_ids"])
                loss = outputs.loss
                
                # åå‘ä¼ æ’­
                loss.backward()
                total_loss += loss.item()
                successful_updates += 1
                
            except Exception as e:
                print(f"å¤„ç†ç¤ºä¾‹æ—¶å‡ºé”™: {str(e)}")
                continue
        
        # æ›´æ–°æ¨¡å‹å‚æ•°
        if successful_updates > 0:
            optimizer = torch.optim.AdamW(model.parameters(), lr=EVOLUTION_CONFIG["learning_rate"])
            optimizer.step()
            optimizer.zero_grad()
            
            avg_loss = total_loss / successful_updates if successful_updates > 0 else 0
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            checkpoint_dir = "./model_checkpoints"
            os.makedirs(checkpoint_dir, exist_ok=True)
            checkpoint_path = f"{checkpoint_dir}/checkpoint_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pt"
            torch.save({
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': avg_loss,
                'examples_count': successful_updates
            }, checkpoint_path)
        else:
            avg_loss = 0
        
        model.eval()
        
        return f"âœ… å¾®è°ƒå®Œæˆï¼Œå¤„ç†äº†{successful_updates}ä¸ªç¤ºä¾‹ï¼Œå¹³å‡æŸå¤±: {avg_loss:.4f}"
        
    except Exception as e:
        model.eval()
        return f"å¾®è°ƒå¤±è´¥: {str(e)}"

# ====== å•ä¸ªé—®é¢˜å¤„ç† ======
def process_single_problem(problem: str, system_prompt: str = None) -> Tuple[bool, Dict]:
    """
    å¤„ç†å•ä¸ªé—®é¢˜çš„å®Œæ•´æµç¨‹
    """
    result = {
        "problem": problem,
        "success": False,
        "generated_code": "",
        "validation_result": "",
        "test_result": "",
        "saved_file": ""
    }
    
    # æ­¥éª¤1: ä½¿ç”¨32Bæ¨¡å‹ç”Ÿæˆä»£ç 
    success, code = call_qwen_api(
        API_CONFIG["qwen_32b_api_url"],
        problem,
        model_name="qwen2.5-coder-32b-instruct"
    )
    
    if not success:
        result["validation_result"] = f"ä»£ç ç”Ÿæˆå¤±è´¥: {code}"
        return False, result
    
    result["generated_code"] = code
    
    # æ­¥éª¤2: è¯­æ³•æ£€æŸ¥
    syntax_ok, syntax_msg = check_code_syntax(code)
    if not syntax_ok:
        result["validation_result"] = f"è¯­æ³•é”™è¯¯: {syntax_msg}"
        return False, result
    
    # æ­¥éª¤3: é€»è¾‘éªŒè¯ï¼ˆ14Bæ¨¡å‹ï¼‰
    logic_ok, logic_msg = validate_code_with_14b(problem, code)
    result["validation_result"] = logic_msg
    
    if not logic_ok:
        return False, result
    
    # æ­¥éª¤4: è¿è¡Œç®€å•æµ‹è¯•
    test_ok, test_msg = run_simple_test(code, problem)
    result["test_result"] = test_msg
    
    if not test_ok:
        print(f"æµ‹è¯•å¤±è´¥ï¼Œä½†ä»ä¿å­˜ç¤ºä¾‹: {test_msg}")
        # ç»§ç»­å¤„ç†ï¼Œå› ä¸ºæœ‰äº›æµ‹è¯•å¯èƒ½è¿‡äºä¸¥æ ¼
    
    # æ­¥éª¤5: ä¿å­˜è®­ç»ƒæ•°æ®
    try:
        saved_file = save_training_example(problem, code, logic_msg)
        result["saved_file"] = saved_file
        result["success"] = True
    except Exception as e:
        result["validation_result"] += f"\nä¿å­˜å¤±è´¥: {str(e)}"
        return False, result
    
    return True, result

# ====== æ‰¹é‡è‡ªæˆ‘æ¼”åŒ–ä¸»å‡½æ•° ======
def batch_self_evolution(problems: List[str], system_prompt: str = None) -> str:
    """
    æ‰¹é‡è‡ªæˆ‘æ¼”åŒ–æµç¨‹
    å¤„ç†ç”¨æˆ·è¾“å…¥ä¸­æå–çš„æ‰€æœ‰é—®é¢˜
    """
    if not problems:
        return "âŒ é”™è¯¯ï¼šæ²¡æœ‰æå–åˆ°æœ‰æ•ˆçš„ç¼–ç¨‹é—®é¢˜ã€‚è¯·ç¡®ä¿é—®é¢˜ç”¨å¼•å·æ‹¬èµ·æ¥ã€‚"
    
    total_problems = len(problems)
    batch_size = EVOLUTION_CONFIG["evolution_batch_size"]
    
    # åˆ›å»ºè¿›åº¦è·Ÿè¸ªå™¨
    tracker = create_progress_tracker(total_problems)
    
    report_lines = []
    report_lines.append("ğŸš€ å¼€å§‹æ‰¹é‡è‡ªæˆ‘æ¼”åŒ–æµç¨‹")
    report_lines.append(f"ğŸ“‹ æå–åˆ° {total_problems} ä¸ªç¼–ç¨‹é—®é¢˜")
    report_lines.append(f"ğŸ“¦ æ‰¹é‡å¤§å°: {batch_size}")
    report_lines.append("=" * 60)
    
    # æ˜¾ç¤ºæå–åˆ°çš„é—®é¢˜
    report_lines.append("ğŸ“ æå–åˆ°çš„é—®é¢˜ï¼š")
    for i, problem in enumerate(problems, 1):
        if len(problem) > 80:
            display_problem = problem[:77] + "..."
        else:
            display_problem = problem
        report_lines.append(f"  {i}. {display_problem}")
    
    report_lines.append("=" * 60)
    
    successful_examples = []
    
    # åˆ†æ‰¹å¤„ç†é—®é¢˜
    for i in range(0, total_problems, batch_size):
        batch = problems[i:i+batch_size]
        batch_num = i // batch_size + 1
        total_batches = (total_problems + batch_size - 1) // batch_size
        
        report_lines.append(f"\nğŸ“ å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches}")
        
        # å¹¶è¡Œå¤„ç†æ‰¹æ¬¡ä¸­çš„é—®é¢˜
        with ThreadPoolExecutor(max_workers=min(batch_size, 4)) as executor:
            future_to_problem = {
                executor.submit(process_single_problem, problem, system_prompt): problem 
                for problem in batch
            }
            
            for future in as_completed(future_to_problem):
                problem = future_to_problem[future]
                try:
                    success, result = future.result(timeout=120)
                    
                    # æ›´æ–°è¿›åº¦
                    if len(problem) > 50:
                        step_name = f"é—®é¢˜: {problem[:47]}..."
                    else:
                        step_name = f"é—®é¢˜: {problem}"
                    
                    progress_report, tracker = update_progress(
                        tracker, step_name, success, 
                        "æˆåŠŸ" if success else result.get("validation_result", "æœªçŸ¥é”™è¯¯")
                    )
                    
                    report_lines.append(progress_report)
                    
                    if success:
                        successful_examples.append(result)
                        report_lines.append(f"  âœ… å·²ä¿å­˜åˆ°: {result['saved_file']}")
                    else:
                        report_lines.append(f"  âŒ å¤±è´¥: {result.get('validation_result', 'æœªçŸ¥é”™è¯¯')[:80]}...")
                        
                except Exception as e:
                    progress_report, tracker = update_progress(tracker, "å¤„ç†å¼‚å¸¸", False, str(e))
                    report_lines.append(progress_report)
        
        report_lines.append("-" * 40)
    
    # å¾®è°ƒæ¨¡å‹
    if successful_examples:
        report_lines.append("\nğŸ¯ å¼€å§‹æ¨¡å‹å¾®è°ƒ...")
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        training_data = []
        for example in successful_examples:
            if example["success"]:
                training_data.append({
                    "instruction": example["problem"],
                    "code": example["generated_code"]
                })
        
        # æ‰§è¡Œå¾®è°ƒ
        fine_tune_result = fine_tune_on_examples(training_data)
        report_lines.append(fine_tune_result)
        
        # ä¿å­˜è®­ç»ƒç»Ÿè®¡
        stats_file = f"{TRAINING_DATA_DIR}/batch_evolution_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        stats = {
            "total_problems": total_problems,
            "successful": len(successful_examples),
            "failed": total_problems - len(successful_examples),
            "timestamp": datetime.now().isoformat(),
            "problems": problems,
            "examples_summary": [
                {
                    "problem": ex["problem"][:100] + "..." if len(ex["problem"]) > 100 else ex["problem"],
                    "success": ex["success"]
                }
                for ex in successful_examples[:10]  # åªä¿å­˜å‰10ä¸ªç¤ºä¾‹çš„æ‘˜è¦
            ]
        }
        
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        report_lines.append(f"ğŸ“Š ç»Ÿè®¡æ•°æ®å·²ä¿å­˜åˆ°: {stats_file}")
    else:
        report_lines.append("\nâš ï¸ æ²¡æœ‰æˆåŠŸçš„ç¤ºä¾‹ï¼Œè·³è¿‡å¾®è°ƒ")
    
    # æœ€ç»ˆæŠ¥å‘Š
    report_lines.append("\n" + "=" * 60)
    report_lines.append("ğŸ‰ æ‰¹é‡è‡ªæˆ‘æ¼”åŒ–æµç¨‹å®Œæˆï¼")
    report_lines.append(f"âœ… æˆåŠŸå¤„ç†: {tracker['success']}/{total_problems}")
    report_lines.append(f"âŒ å¤±è´¥: {tracker['failed']}/{total_problems}")
    report_lines.append(f"â±ï¸ æ€»ç”¨æ—¶: {time.time() - tracker['start_time']:.1f}ç§’")
    
    if successful_examples:
        report_lines.append(f"ğŸ’¾ æ¨¡å‹å·²æ›´æ–°ï¼Œæ£€æŸ¥ç‚¹å·²ä¿å­˜")
    
    return "\n".join(report_lines)

# ====== æ¨¡å‹åŠ è½½å‡½æ•° ======
def load_model(model_path=None):
    """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
    global model, tokenizer, device
    
    if model_path is None or model_path.strip() == "":
        model_path = DEFAULT_MODEL_PATH
    
    if not os.path.exists(model_path):
        return f"é”™è¯¯ï¼šæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}"
    
    try:
        print(f"æ­£åœ¨ä»æœ¬åœ°è·¯å¾„åŠ è½½æ¨¡å‹: {model_path}")
        
        # åŠ è½½åˆ†è¯å™¨
        tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)
        
        # ç¡®å®šè®¾å¤‡
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")
        
        # åŠ è½½æ¨¡å‹
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            local_files_only=True,
            dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        model = model.to(device)
        model.eval()
        
        return f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼\næ¨¡å‹è·¯å¾„: {model_path}\nä½¿ç”¨è®¾å¤‡: {device}"
        
    except Exception as e:
        return f"âŒ åŠ è½½æ¨¡å‹æ—¶å‡ºé”™ï¼š{str(e)}"

# ====== ä¸»ç”Ÿæˆå‡½æ•° ======
def generate_code(prompt, system_prompt, max_tokens, temperature, top_p, enable_evolution=True):
    """ç”Ÿæˆä»£ç çš„ä¸»å‡½æ•°"""
    global model, tokenizer
    
    if model is None or tokenizer is None:
        return "é”™è¯¯ï¼šæ¨¡å‹å°šæœªåŠ è½½ï¼Œè¯·å…ˆç‚¹å‡»'åŠ è½½æ¨¡å‹'æŒ‰é’®ã€‚", ""
    
    if not prompt or prompt.strip() == "":
        return "é”™è¯¯ï¼šè¯·è¾“å…¥ä»£ç ç”Ÿæˆæç¤ºã€‚", ""
    
    # æ£€æµ‹æ˜¯å¦è§¦å‘è‡ªæˆ‘æ¼”åŒ–æ¨¡å¼
    should_evolve, extracted_problems = detect_evolution_mode(prompt)
    
    if should_evolve and enable_evolution:
        # è¿›å…¥è‡ªæˆ‘æ¼”åŒ–åˆ†æ”¯
        if extracted_problems:
            # æ‰¹é‡å¤„ç†æå–åˆ°çš„é—®é¢˜
            evolution_status = batch_self_evolution(extracted_problems, system_prompt)
            return evolution_status, ""
        else:
            # æ²¡æœ‰æå–åˆ°é—®é¢˜ï¼Œå¯èƒ½æ˜¯å•é—®é¢˜è‡ªæˆ‘æ¼”åŒ–
            try:
                # æå–å•ä¸ªé—®é¢˜ï¼ˆç§»é™¤æ¼”åŒ–å…³é”®è¯ï¼‰
                clean_prompt = prompt
                for keyword in EVOLUTION_CONFIG["evolution_keywords"]:
                    clean_prompt = clean_prompt.replace(keyword, "")
                clean_prompt = clean_prompt.strip()
                
                if clean_prompt:
                    success, result = process_single_problem(clean_prompt, system_prompt)
                    
                    if success:
                        status = f"âœ… å•é—®é¢˜è‡ªæˆ‘æ¼”åŒ–å®Œæˆï¼\n"
                        status += f"ğŸ“ å·²ä¿å­˜è®­ç»ƒæ•°æ®åˆ°: {result['saved_file']}\n"
                        
                        # å¾®è°ƒæ¨¡å‹
                        fine_tune_result = fine_tune_on_examples([{
                            "instruction": result["problem"],
                            "code": result["generated_code"]
                        }])
                        status += f"ğŸ¯ {fine_tune_result}"
                        
                        return status, result["generated_code"]
                    else:
                        return f"âŒ è‡ªæˆ‘æ¼”åŒ–å¤±è´¥:\n{result['validation_result']}", ""
                else:
                    return "âŒ é”™è¯¯ï¼šè¯·æä¾›è¦æ¼”åŒ–çš„å…·ä½“é—®é¢˜ã€‚", ""
                    
            except Exception as e:
                return f"è‡ªæˆ‘æ¼”åŒ–æ—¶å‡ºé”™ï¼š{str(e)}", ""
    else:
        # æ­£å¸¸ä»£ç ç”Ÿæˆåˆ†æ”¯
        try:
            messages = [
                {"role": "system", "content": system_prompt if system_prompt else "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¼–ç¨‹åŠ©æ‰‹ï¼Œæ“…é•¿ç¼–å†™å’Œè§£é‡Šä»£ç ã€‚"},
                {"role": "user", "content": prompt},
            ]
            
            text = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            model_inputs = tokenizer([text], return_tensors="pt").to(device)
            
            with torch.no_grad():
                generated_ids = model.generate(
                    **model_inputs,
                    max_new_tokens=int(max_tokens),
                    temperature=float(temperature),
                    top_p=float(top_p),
                    do_sample=True
                )
            
            generated_ids = [
                output_ids[len(input_ids):] 
                for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
            ]
            
            response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
            return "âœ… ä»£ç ç”Ÿæˆå®Œæˆ", response
            
        except Exception as e:
            return f"ç”Ÿæˆä»£ç æ—¶å‡ºé”™ï¼š{str(e)}", ""

# ====== æŸ¥çœ‹è®­ç»ƒæ•°æ® ======
def list_training_data(limit: int = 20):
    """åˆ—å‡ºè®­ç»ƒæ•°æ®"""
    if not os.path.exists(TRAINING_DATA_DIR):
        return "æš‚æ— è®­ç»ƒæ•°æ®"
    
    files = [f for f in os.listdir(TRAINING_DATA_DIR) if f.endswith('.json')]
    if not files:
        return "æš‚æ— è®­ç»ƒæ•°æ®"
    
    files.sort(reverse=True)  # æŒ‰æ—¶é—´å€’åº
    files = files[:limit]
    
    result = f"ğŸ“š æœ€è¿‘ {len(files)} ä¸ªè®­ç»ƒæ ·æœ¬ï¼š\n\n"
    
    for i, file in enumerate(files, 1):
        file_path = os.path.join(TRAINING_DATA_DIR, file)
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                instruction = data.get('instruction', 'æœªçŸ¥æŒ‡ä»¤')
                timestamp = data.get('timestamp', 'æœªçŸ¥æ—¶é—´')
                
                result += f"{i}. {file}\n"
                result += f"   æŒ‡ä»¤: {instruction[:80]}...\n"
                result += f"   æ—¶é—´: {timestamp}\n"
                result += f"   ç±»å‹: {data.get('metadata', {}).get('problem_type', 'general')}\n"
                result += "   ---\n"
        except:
            result += f"{i}. {file} (è¯»å–å¤±è´¥)\n"
    
    return result

# ====== Gradioç•Œé¢ ======
with gr.Blocks(title="Qwen2.5-Coder æ‰¹é‡è‡ªæˆ‘æ¼”åŒ–ç³»ç»Ÿ", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸ¤– Qwen2.5-Coder æ‰¹é‡è‡ªæˆ‘æ¼”åŒ–ç³»ç»Ÿ")
    gr.Markdown("""
    ## ğŸš€ åŠŸèƒ½ç‰¹æ€§ï¼š
    1. **æ™®é€šä»£ç ç”Ÿæˆ**ï¼šä½¿ç”¨æœ¬åœ°1.5Bæ¨¡å‹ç”Ÿæˆä»£ç 
    2. **æ‰¹é‡è‡ªæˆ‘æ¼”åŒ–**ï¼šè¾“å…¥åŒ…å«å¤šä¸ªå¼•å·å†…çš„é—®é¢˜ï¼Œç³»ç»Ÿè‡ªåŠ¨æå–å¹¶æ‰¹é‡è®­ç»ƒ
    3. **æ™ºèƒ½é—®é¢˜æå–**ï¼šè‡ªåŠ¨ä»æ–‡æœ¬ä¸­æå–å¼•å·å†…çš„ç¼–ç¨‹é—®é¢˜
    """)
    
    with gr.Row():
        with gr.Column(scale=1):
            gr.Markdown("### ğŸ“ æ¨¡å‹è®¾ç½®")
            model_path_input = gr.Textbox(
                label="æ¨¡å‹è·¯å¾„", value=DEFAULT_MODEL_PATH, lines=1
            )
            load_btn = gr.Button("ğŸ”„ åŠ è½½æ¨¡å‹", variant="primary", size="lg")
            load_status = gr.Textbox(label="æ¨¡å‹çŠ¶æ€", interactive=False, lines=3)
            
            with gr.Accordion("ğŸ”‘ APIè®¾ç½®", open=False):
                api_key_input = gr.Textbox(
                    label="APIå¯†é’¥", value=API_CONFIG["api_key"], type="password", lines=1
                )
                api_32b_url = gr.Textbox(
                    label="32B APIåœ°å€", value=API_CONFIG["qwen_32b_api_url"], lines=1
                )
                api_14b_url = gr.Textbox(
                    label="14B APIåœ°å€", value=API_CONFIG["qwen_14b_api_url"], lines=1
                )
            
            with gr.Accordion("âš™ï¸ è‡ªæˆ‘æ¼”åŒ–è®¾ç½®", open=False):
                enable_evolution = gr.Checkbox(
                    label="å¯ç”¨è‡ªæˆ‘æ¼”åŒ–", value=EVOLUTION_CONFIG["enable_self_evolution"]
                )
                evolution_keywords = gr.Textbox(
                    label="æ¼”åŒ–å…³é”®è¯", value=",".join(EVOLUTION_CONFIG["evolution_keywords"]), lines=2
                )
                batch_size = gr.Slider(
                    label="æ‰¹é‡å¤§å°", minimum=1, maximum=10, value=EVOLUTION_CONFIG["evolution_batch_size"], step=1
                )
                learning_rate = gr.Slider(
                    label="å­¦ä¹ ç‡", minimum=1e-6, maximum=1e-3, value=EVOLUTION_CONFIG["learning_rate"], step=1e-6
                )
            
            with gr.Accordion("ğŸ“Š æ•°æ®ç®¡ç†", open=False):
                with gr.Row():
                    view_data_btn = gr.Button("æŸ¥çœ‹è®­ç»ƒæ•°æ®", variant="secondary")
                    test_extraction_btn = gr.Button("æµ‹è¯•é—®é¢˜æå–", variant="secondary")
                
                training_data_view = gr.Textbox(
                    label="è®­ç»ƒæ•°æ®", interactive=False, lines=10
                )
            
            with gr.Accordion("âš™ï¸ ç”Ÿæˆè®¾ç½®", open=False):
                system_prompt_input = gr.Textbox(
                    label="ç³»ç»Ÿæç¤ºè¯",
                    value="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¼–ç¨‹åŠ©æ‰‹ï¼Œæ“…é•¿ç¼–å†™å’Œè§£é‡Šä»£ç ã€‚",
                    lines=2
                )
                max_tokens_input = gr.Slider(
                    label="æœ€å¤§tokenæ•°", minimum=50, maximum=2048, value=512, step=50
                )
                temperature_input = gr.Slider(
                    label="Temperature", minimum=0.1, maximum=2.0, value=0.7, step=0.1
                )
                top_p_input = gr.Slider(
                    label="Top-p", minimum=0.1, maximum=1.0, value=0.9, step=0.05
                )
        
        with gr.Column(scale=2):
            gr.Markdown("### ğŸ’» ä»£ç ç”Ÿæˆä¸è‡ªæˆ‘æ¼”åŒ–")
            
            mode_indicator = gr.Markdown("**å½“å‰æ¨¡å¼ï¼š** ç­‰å¾…è¾“å…¥...")
            
            # ç¤ºä¾‹è¾“å…¥
            example_input = '''è¯·è‡ªæˆ‘æ¼”åŒ–
"Write a function to find the minimum cost path to reach (m, n) from (0, 0) for the given cost matrix cost[][] and a position (m, n) in cost[][]."
"Write a function to find the similar elements from the given two tuple lists."
"Write a python function to identify non-prime numbers."
"Write a function to find the largest integers from a given list of numbers using heap queue algorithm."
"Write a function to find the number of ways to fill it with 2 x 1 dominoes for the given 3 x n board."'''
            
            prompt_input = gr.Textbox(
                label="è¾“å…¥æç¤ºè¯",
                placeholder=example_input,
                lines=10,
                value=example_input
            )
            
            with gr.Row():
                generate_btn = gr.Button("âœ¨ ç”Ÿæˆä»£ç ", variant="primary", size="lg")
                evolve_btn = gr.Button("ğŸš€ æ‰§è¡Œè‡ªæˆ‘æ¼”åŒ–", variant="stop", size="lg")
            
            status_output = gr.Textbox(
                label="æ‰§è¡ŒçŠ¶æ€", interactive=False, lines=12
            )
            
            code_output = gr.Code(
                label="ç”Ÿæˆçš„ä»£ç ", language="python", lines=20
            )
    
    # ====== äº‹ä»¶å¤„ç† ======
    def update_api_config(api_key, api_32b, api_14b):
        global API_CONFIG
        API_CONFIG["api_key"] = api_key
        API_CONFIG["qwen_32b_api_url"] = api_32b
        API_CONFIG["qwen_14b_api_url"] = api_14b
        return "âœ… APIé…ç½®å·²æ›´æ–°"
    
    def update_evolution_config(enable, keywords, batch, lr):
        global EVOLUTION_CONFIG
        EVOLUTION_CONFIG["enable_self_evolution"] = enable
        EVOLUTION_CONFIG["evolution_keywords"] = [k.strip() for k in keywords.split(",") if k.strip()]
        EVOLUTION_CONFIG["evolution_batch_size"] = batch
        EVOLUTION_CONFIG["learning_rate"] = lr
        return "âœ… è‡ªæˆ‘æ¼”åŒ–é…ç½®å·²æ›´æ–°"
    
    def detect_mode(prompt):
        if not prompt:
            return "**å½“å‰æ¨¡å¼ï¼š** ç­‰å¾…è¾“å…¥..."
        
        should_evolve, problems = detect_evolution_mode(prompt)
        
        if should_evolve:
            if problems:
                return f"**å½“å‰æ¨¡å¼ï¼š** ğŸš€ æ‰¹é‡è‡ªæˆ‘æ¼”åŒ–æ¨¡å¼ï¼ˆæ£€æµ‹åˆ°{len(problems)}ä¸ªé—®é¢˜ï¼‰"
            else:
                return "**å½“å‰æ¨¡å¼ï¼š** ğŸ”„ å•é—®é¢˜è‡ªæˆ‘æ¼”åŒ–æ¨¡å¼"
        
        return "**å½“å‰æ¨¡å¼ï¼š** ğŸ’» æ™®é€šä»£ç ç”Ÿæˆæ¨¡å¼"
    
    # æµ‹è¯•é—®é¢˜æå–
    def test_problem_extraction(prompt):
        should_evolve, problems = detect_evolution_mode(prompt)
        
        if not should_evolve:
            return "æœªæ£€æµ‹åˆ°è‡ªæˆ‘æ¼”åŒ–å…³é”®è¯ã€‚"
        
        if not problems:
            return "æ£€æµ‹åˆ°è‡ªæˆ‘æ¼”åŒ–å…³é”®è¯ï¼Œä½†æ²¡æœ‰æå–åˆ°é—®é¢˜ã€‚"
        
        result = f"âœ… æ£€æµ‹åˆ°è‡ªæˆ‘æ¼”åŒ–æ¨¡å¼\n"
        result += f"ğŸ“‹ æå–åˆ° {len(problems)} ä¸ªé—®é¢˜ï¼š\n\n"
        
        for i, problem in enumerate(problems, 1):
            result += f"{i}. {problem}\n"
        
        result += f"\næç¤ºï¼šç‚¹å‡»'æ‰§è¡Œè‡ªæˆ‘æ¼”åŒ–'æŒ‰é’®å¼€å§‹æ‰¹é‡è®­ç»ƒã€‚"
        return result
    
    # ç»‘å®šäº‹ä»¶
    load_btn.click(
        fn=load_model,
        inputs=model_path_input,
        outputs=load_status
    )
    
    generate_btn.click(
        fn=generate_code,
        inputs=[
            prompt_input, system_prompt_input, max_tokens_input, 
            temperature_input, top_p_input, enable_evolution
        ],
        outputs=[status_output, code_output]
    ).then(
        fn=detect_mode,
        inputs=prompt_input,
        outputs=mode_indicator
    )
    
    evolve_btn.click(
        fn=generate_code,
        inputs=[
            prompt_input, system_prompt_input, max_tokens_input, 
            temperature_input, top_p_input, enable_evolution
        ],
        outputs=[status_output, code_output]
    ).then(
        fn=detect_mode,
        inputs=prompt_input,
        outputs=mode_indicator
    )
    
    # APIé…ç½®æ›´æ–°
    api_key_input.change(
        fn=update_api_config,
        inputs=[api_key_input, api_32b_url, api_14b_url],
        outputs=gr.Textbox(visible=False)
    )
    
    # æ¼”åŒ–é…ç½®æ›´æ–°
    enable_evolution.change(
        fn=update_evolution_config,
        inputs=[enable_evolution, evolution_keywords, batch_size, learning_rate],
        outputs=gr.Textbox(visible=False)
    )
    
    # æŸ¥çœ‹è®­ç»ƒæ•°æ®
    view_data_btn.click(
        fn=list_training_data,
        outputs=training_data_view
    )
    
    # æµ‹è¯•é—®é¢˜æå–
    test_extraction_btn.click(
        fn=test_problem_extraction,
        inputs=prompt_input,
        outputs=training_data_view
    )
    
    # å®æ—¶æ£€æµ‹æ¨¡å¼
    prompt_input.change(
        fn=detect_mode,
        inputs=prompt_input,
        outputs=mode_indicator
    )
    
    # ç¤ºä¾‹æç¤ºè¯
    gr.Examples(
        examples=[
            [example_input],
            ["è¯·è‡ªæˆ‘æ¼”åŒ–\n\"ç”¨Pythonå®ç°ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•ã€‚\"\n\"ç”¨Pythonå®ç°ä¸€ä¸ªäºŒå‰æ ‘çš„éå†ç®—æ³•ã€‚\""],
            ["ç”¨Pythonç¼–å†™ä¸€ä¸ªç®€å•çš„HTTPæœåŠ¡å™¨ã€‚"],
        ],
        inputs=prompt_input,
        outputs=[mode_indicator]
    )
    
    # ä½¿ç”¨è¯´æ˜
    gr.Markdown("""
    ## ğŸ“– ä½¿ç”¨è¯´æ˜ï¼š
    
    ### 1. æ™®é€šä»£ç ç”Ÿæˆï¼š
    - è¾“å…¥æ™®é€šçš„ä»£ç ç”Ÿæˆæç¤º
    - ç‚¹å‡»"ç”Ÿæˆä»£ç "æŒ‰é’®
    
    ### 2. æ‰¹é‡è‡ªæˆ‘æ¼”åŒ–ï¼š
    - åœ¨è¾“å…¥ä¸­åŒ…å«"è‡ªæˆ‘æ¼”åŒ–"å…³é”®è¯
    - ç”¨**åŒå¼•å·**æ‹¬èµ·æ¯ä¸ªç¼–ç¨‹é—®é¢˜
    - æ¯ä¸ªé—®é¢˜å ä¸€è¡Œæˆ–ä½¿ç”¨åˆ†éš”ç¬¦
    - ç‚¹å‡»"æ‰§è¡Œè‡ªæˆ‘æ¼”åŒ–"æŒ‰é’®
    
    ### 3. è¾“å…¥æ ¼å¼ç¤ºä¾‹ï¼š
    ```
    è¯·è‡ªæˆ‘æ¼”åŒ–
    "Write a function to find the minimum cost path..."
    "Write a function to find the similar elements..."
    "Write a python function to identify non-prime numbers..."
    ```
    
    ### 4. ç³»ç»Ÿæµç¨‹ï¼š
    1. æ£€æµ‹"è‡ªæˆ‘æ¼”åŒ–"å…³é”®è¯
    2. æå–æ‰€æœ‰å¼•å·å†…çš„é—®é¢˜
    3. å¯¹æ¯ä¸ªé—®é¢˜ï¼š
       - è°ƒç”¨32b APIç”Ÿæˆä»£ç 
       - 14Bæ¨¡å‹éªŒè¯ä»£ç é€»è¾‘
       - è¯­æ³•æ£€æŸ¥
       - ä¿å­˜è®­ç»ƒæ•°æ®
    4. ç”¨æ‰€æœ‰æˆåŠŸçš„é—®é¢˜å¾®è°ƒæœ¬åœ°1.5Bæ¨¡å‹
    5. è¿”å›å¤„ç†æŠ¥å‘Š
    
    ### 5. æ³¨æ„äº‹é¡¹ï¼š
    - APIå¯†é’¥éœ€è¦æ­£ç¡®é…ç½®
    - è‡ªæˆ‘æ¼”åŒ–è¿‡ç¨‹å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´
    - æ¨¡å‹å¾®è°ƒåä¼šä¿å­˜æ£€æŸ¥ç‚¹
    - è®­ç»ƒæ•°æ®ä¿å­˜åœ¨`./evolution_training_data/`ç›®å½•
    """)

if __name__ == "__main__":
    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    os.makedirs(TRAINING_DATA_DIR, exist_ok=True)
    os.makedirs("./model_checkpoints", exist_ok=True)
    
    # å¯åŠ¨ Gradio ç•Œé¢
    demo.launch(
        share=False, 
        server_name="0.0.0.0", 
        server_port=7860,
        show_api=False
    )