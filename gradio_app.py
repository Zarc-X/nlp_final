# ä½¿ç”¨ Qwen2.5-Coder æ¨¡å‹çš„ Gradio ç•Œé¢
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import gradio as gr

# å…¨å±€å˜é‡å­˜å‚¨æ¨¡å‹å’Œåˆ†è¯å™¨
model = None
tokenizer = None
device = None

def load_model():
    """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
    global model, tokenizer, device
    
    # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    model_name = "Qwen/Qwen2.5-Coder-0.5B-Instruct"
    
    print("æ­£åœ¨åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    # ç¡®å®šè®¾å¤‡
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        low_cpu_mem_usage=True
    )
    model = model.to(device)
    model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    print("æ¨¡å‹åŠ è½½å®Œæˆï¼")
    return "æ¨¡å‹åŠ è½½å®Œæˆï¼"

def generate_code(prompt, system_prompt, max_tokens, temperature, top_p):
    """ç”Ÿæˆä»£ç çš„å‡½æ•°"""
    if model is None or tokenizer is None:
        return "é”™è¯¯ï¼šæ¨¡å‹å°šæœªåŠ è½½ï¼Œè¯·å…ˆç‚¹å‡»'åŠ è½½æ¨¡å‹'æŒ‰é’®ã€‚"
    
    if not prompt or prompt.strip() == "":
        return "é”™è¯¯ï¼šè¯·è¾“å…¥ä»£ç ç”Ÿæˆæç¤ºã€‚"
    
    try:
        # å‡†å¤‡å¯¹è¯æ¶ˆæ¯
        messages = [
            {"role": "system", "content": system_prompt if system_prompt else "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¼–ç¨‹åŠ©æ‰‹ï¼Œæ“…é•¿ç¼–å†™å’Œè§£é‡Šä»£ç ã€‚"},
            {"role": "user", "content": prompt},
        ]
        
        # åº”ç”¨èŠå¤©æ¨¡æ¿
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        # å°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥
        model_inputs = tokenizer([text], return_tensors="pt").to(device)
        
        # ç”Ÿæˆä»£ç 
        with torch.no_grad():
            generated_ids = model.generate(
                **model_inputs,
                max_new_tokens=int(max_tokens),
                temperature=float(temperature),
                top_p=float(top_p),
                do_sample=True
            )
        
        # æå–ç”Ÿæˆçš„æ–‡æœ¬ï¼ˆå»æ‰è¾“å…¥éƒ¨åˆ†ï¼‰
        generated_ids = [
            output_ids[len(input_ids):] 
            for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
        ]
        
        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        return response
        
    except Exception as e:
        return f"ç”Ÿæˆä»£ç æ—¶å‡ºé”™ï¼š{str(e)}"

# åˆ›å»º Gradio ç•Œé¢
with gr.Blocks(title="Qwen2.5-Coder ä»£ç ç”Ÿæˆå™¨", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸ¤– Qwen2.5-Coder ä»£ç ç”Ÿæˆå™¨")
    gr.Markdown("ä½¿ç”¨ Qwen2.5-Coder æ¨¡å‹ç”Ÿæˆä»£ç ã€‚è¯·å…ˆåŠ è½½æ¨¡å‹ï¼Œç„¶åè¾“å…¥æç¤ºè¯ç”Ÿæˆä»£ç ã€‚")
    
    with gr.Row():
        with gr.Column():
            load_btn = gr.Button("ğŸ”„ åŠ è½½æ¨¡å‹", variant="primary", size="lg")
            load_status = gr.Textbox(label="æ¨¡å‹çŠ¶æ€", interactive=False)
            
            with gr.Accordion("âš™ï¸ é«˜çº§è®¾ç½®", open=False):
                system_prompt_input = gr.Textbox(
                    label="ç³»ç»Ÿæç¤ºè¯",
                    value="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¼–ç¨‹åŠ©æ‰‹ï¼Œæ“…é•¿ç¼–å†™å’Œè§£é‡Šä»£ç ã€‚",
                    lines=2,
                    placeholder="è¾“å…¥ç³»ç»Ÿæç¤ºè¯..."
                )
                max_tokens_input = gr.Slider(
                    label="æœ€å¤§ç”Ÿæˆtokenæ•°",
                    minimum=50,
                    maximum=2048,
                    value=512,
                    step=50
                )
                temperature_input = gr.Slider(
                    label="Temperature (åˆ›é€ æ€§)",
                    minimum=0.1,
                    maximum=2.0,
                    value=0.7,
                    step=0.1
                )
                top_p_input = gr.Slider(
                    label="Top-p (æ ¸é‡‡æ ·)",
                    minimum=0.1,
                    maximum=1.0,
                    value=0.9,
                    step=0.05
                )
        
        with gr.Column():
            prompt_input = gr.Textbox(
                label="ä»£ç ç”Ÿæˆæç¤º",
                placeholder="ä¾‹å¦‚ï¼šè¯·ç”¨Pythonç¼–å†™ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•ã€‚",
                lines=5
            )
            generate_btn = gr.Button("âœ¨ ç”Ÿæˆä»£ç ", variant="primary", size="lg")
            output = gr.Code(
                label="ç”Ÿæˆçš„ä»£ç ",
                language="python",
                lines=20
            )
    
    # ç»‘å®šäº‹ä»¶
    load_btn.click(
        fn=load_model,
        outputs=load_status
    )
    
    generate_btn.click(
        fn=generate_code,
        inputs=[prompt_input, system_prompt_input, max_tokens_input, temperature_input, top_p_input],
        outputs=output
    )
    
    # ç¤ºä¾‹æç¤ºè¯
    gr.Examples(
        examples=[
            ["è¯·ç”¨Pythonç¼–å†™ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•ã€‚"],
            ["ç”¨Pythonå®ç°ä¸€ä¸ªç®€å•çš„HTTPæœåŠ¡å™¨ã€‚"],
            ["å†™ä¸€ä¸ªå‡½æ•°æ¥è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„ç¬¬né¡¹ã€‚"],
            ["ç”¨Pythonå®ç°ä¸€ä¸ªç®€å•çš„è®¡ç®—å™¨ç±»ã€‚"],
        ],
        inputs=prompt_input
    )

if __name__ == "__main__":
    # å¯åŠ¨ Gradio ç•Œé¢
    demo.launch(share=False, server_name="0.0.0.0", server_port=7860)

